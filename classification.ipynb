{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from transformers import AutoImageProcessor, ResNetForImageClassification,ResNetConfig\n",
    "import torch\n",
    "from abc import ABC,abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check if layers are actually being trained\n",
    "#TODO add more models\n",
    "#TODO compare metrics\n",
    "#TODO test out prediction loop\n",
    "#TODO learn how to use tensorboard\n",
    "\n",
    "#TODO week2 Balance Dataset\n",
    "#TODO day2 Implement checkpointing and checkpoint loading\n",
    "#TODO day2 implement an actually good and modular pipeline\n",
    "#TODO day2 EDA + explore augments\n",
    "#TODO day2 normalize and test out mean/median/std of pixel values\n",
    "#TODO day2 Actually implement additional layers and finish what i started\n",
    "#TODO Verify data is sent correctly (esp labels)\n",
    "\n",
    "#TODO day2 implement localization/segmentation + research more abt osteopenia\n",
    "#interesting models: yolo v5, medsam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###DATASET PREP###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectPath = r\"Osteoporosis Knee X-ray\"\n",
    "pathList = []\n",
    "labelList = []\n",
    "dirList = os.listdir(projectPath)[:3]\n",
    "for idx, x in enumerate(dirList):\n",
    "    for xx in os.listdir(f\"{projectPath}/{x}\"):\n",
    "        pathList.append(f\"{projectPath}/{x}/{xx}\")\n",
    "        labelList.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(1024, 1024)': 63,\n",
       " '(2180, 2660)': 61,\n",
       " '(2660, 2180)': 10,\n",
       " '(2430, 1994)': 94,\n",
       " '(2386, 1994)': 1,\n",
       " '(1994, 2430)': 2,\n",
       " '(2430, 1910)': 2,\n",
       " '(2402, 1994)': 1,\n",
       " '(2378, 1994)': 2,\n",
       " '(2362, 1994)': 1,\n",
       " '(2430, 1958)': 1,\n",
       " '(2398, 1994)': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "imageSizes = {}\n",
    "for x in pathList:\n",
    "    img = Image.open(x).size\n",
    "    try:\n",
    "        imageSizes[str(img)] = imageSizes[str(img)] + 1\n",
    "    except KeyError:\n",
    "        imageSizes[str(img)] = 1\n",
    "imageSizes #varied image sizes, have to resize to 1024,1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 154, 49)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelList.count(0),labelList.count(1),labelList.count(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "\n",
    "class OsteoTorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, itemsPath:list, labels:list, transform=None):\n",
    "        \n",
    "        self.itemsPath = itemsPath\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itemsPath)\n",
    "\n",
    "    def __getitem__(self,idx)->tuple[Image.Image,int]:\n",
    "        # if torch.is_tensor(idx):\n",
    "        #     idx = idx.tolist()\n",
    "        image = Image.open(self.itemsPath[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return torchvision.transforms.functional.pil_to_tensor(image), self.labels[idx]\n",
    "        \n",
    "    \n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.Resize([244,244])\n",
    "                                            # ,torchvision.transforms.Grayscale()\n",
    "                                            ])\n",
    "osteoDataset = OsteoTorchDataset(pathList,labelList,transform)\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "train,val = torch.utils.data.random_split(osteoDataset,[0.8,0.2])#MAY BUG\n",
    "trainSampler = WeightedRandomSampler(weights=[0.4,0.2,0.4],num_samples=len(train),replacement=True)\n",
    "# valSampler = WeightedRandomSampler(weights=[0.4,0.2,0.4],num_samples=,replacement=True)\n",
    "\n",
    "trainLoader = DataLoader(train, batch_size = 16,shuffle=False,num_workers=0)\n",
    "valLoader = DataLoader(val, batch_size = 16,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 2, 1, 1, 1, 2, 1, 2]),\n",
       " tensor([2, 1, 1, 1, 1, 1, 0, 0, 1, 0, 2, 0, 1, 1, 0])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y for x,y in valLoader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataWrapper import DataWrapper\n",
    "# BUG still dosent work, does not fix the Num_workers issue\n",
    "# loader = DataWrapper(pathList,labelList, batch_size = 16,shuffle=False,num_workers=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models here\n",
    "from models import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\assaw\\.conda\\envs\\AI\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in C:\\Users\\assaw/.cache\\torch\\hub\\pytorch_vision_v0.19.0\n",
      "C:\\Users\\assaw\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\assaw\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\assaw/.cache\\torch\\hub\\pytorch_vision_v0.19.0\n",
      "Using cache found in C:\\Users\\assaw/.cache\\torch\\hub\\pytorch_vision_v0.19.0\n",
      "C:\\Users\\assaw\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_M_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no fc detected, switcing to provided outputSize\n",
      "no fc detected, switcing to provided outputSize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\assaw/.cache\\torch\\hub\\pytorch_vision_v0.19.0\n",
      "C:\\Users\\assaw\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torcheval.metrics.functional import multiclass_f1_score,multiclass_confusion_matrix,multiclass_accuracy\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import mode\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "#FIXME implement proper instancing to support multiple models\n",
    "#FIXME identify bottleneck\n",
    "#FIXME clean up my fucking code ffs its so UGLY\n",
    "\n",
    "class ExperimentModel(L.LightningModule):\n",
    "    def __init__(self,pretrainedModel,pretrainedPreprocess,outputSize,resize = False,pretrained = False) -> None:\n",
    "        super().__init__()\n",
    "        if type(pretrainedModel) != str:\n",
    "            print(\"using model as-is\")\n",
    "            self.model = pretrainedModel\n",
    "        else:\n",
    "            self.model = torch.hub.load('pytorch/vision:v0.19.0', pretrainedModel, pretrained=pretrained)\n",
    "            \n",
    "        # if pretrainedPreprocess:\n",
    "        #     self.preprocess = pretrainedPreprocess  \n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        self.resize = resize\n",
    "\n",
    "        if self.resize:\n",
    "            try: \n",
    "                self.linear1 = torch.nn.Linear(self.model.fc.out_features, 512)\n",
    "            except AttributeError:\n",
    "                print(\"no fc detected, switcing to provided outputSize\")\n",
    "                self.linear1 = torch.nn.Linear(outputSize, 512)\n",
    "\n",
    "            self.linear2 = torch.nn.Linear(512, 32)\n",
    "            self.linear3 = torch.nn.Linear(32, 3) \n",
    "            #FIXME add softmax\n",
    "\n",
    "        self.valLog = []\n",
    "        self.epoch = []\n",
    "        self.valPreds = []\n",
    "        self.valLabels = []\n",
    "        self.valScore = []\n",
    "\n",
    "        self.bestValPreds = []\n",
    "        self.bestValLabels = []\n",
    "        self.bestValScore = 0\n",
    "        \n",
    "    def forward(self,input):\n",
    "        out = self.model(input)\n",
    "        if self.resize:\n",
    "            out = self.linear1(out)\n",
    "            out = self.linear2(out)\n",
    "            out = self.linear3(out)\n",
    "        return out\n",
    "        \n",
    "    def training_step(self,batch):\n",
    "        data,label = batch\n",
    "        output = self(data.float())\n",
    "        loss= F.cross_entropy(output,label)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> None:\n",
    "        data,label = batch\n",
    "        output = self(data.float())\n",
    "        loss= F.cross_entropy(output,label)\n",
    "        self.valLabels.append(label)\n",
    "        self.valPreds.append(output)\n",
    "        self.valScore.append(multiclass_f1_score(output,label,num_classes=3))\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_F1\", multiclass_f1_score(output,label,num_classes=3), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", multiclass_accuracy(output,label,num_classes=3), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        if len(self.valScore) == 2:\n",
    "            return None\n",
    "        \n",
    "        accuracy = torch.stack(self.valScore).mean()\n",
    "        # self.log(\"cust\",self.linear3.weight.sum(),logger=True)\n",
    "        if accuracy > self.bestValScore:\n",
    "            print(\"updated best score--------------------------------------------------------\")\n",
    "            self.bestValScore = accuracy\n",
    "            self.bestValPreds = torch.cat(self.valPreds)\n",
    "            self.bestValLabels = torch.cat(self.valLabels)\n",
    "\n",
    "        print(f\"\\n Validation accuracy: {accuracy}\")\n",
    "        print(f\"debug: {len(self.valScore)}\")\n",
    "        print(f\"bestValScore: {self.bestValScore}\")\n",
    "        \n",
    "        self.valPreds = []\n",
    "        self.valLabels = []\n",
    "        self.valScore = []\n",
    "\n",
    "        # confusion_matrix = MulticlassConfusionMatrix(num_classes=3, threshold=0.05)\n",
    "        # confusion_matrix(y_hat, y.int())\n",
    "\n",
    "        # confusion_matrix_computed = confusion_matrix.compute().detach().cpu().numpy().astype(int)\n",
    "\n",
    "        # df_cm = pd.DataFrame(confusion_matrix_computed)\n",
    "        # plt.figure(figsize = (10,7))\n",
    "        # fig_ = sns.heatmap(df_cm, annot=True, cmap='Spectral').get_figure()\n",
    "        # plt.close(fig_)\n",
    "        # self.log.experiment.add_figure(\"Confusion matrix\", fig_, self.current_epoch)\n",
    "\n",
    "    def get_self_variables(self):\n",
    "        linear_variables = [x for x in dir(self) if 'linear' in x]\n",
    "        return [getattr(self, x, None).parameters() for x in linear_variables]\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(),lr=0.001)\n",
    "        # return torch.optim.AdamW(itertools.chain(self.model.parameters(),*self.get_self_variables()), lr=0.001)\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        data,label = batch\n",
    "        return self(data.float())\n",
    "\n",
    "# class ResNetCallbacks(L.Callback): #TODO implement callbacks\n",
    "#     def on_validation_epoch_end(trainer, pl_module):\n",
    "#         valLog.append(sum(self.cumLog)/len(self.cumlog))\n",
    "\n",
    "    \n",
    "# alexNet = ExperimentModel(AlexNet(num_classes=1000),None,1000,True)\n",
    "# alexNetNonMod = ExperimentModel(AlexNet(num_classes=3),None,3)\n",
    "# alexNet = ExperimentModel(AlexNet(num_classes=3),None,3)\n",
    "resnet50 = ExperimentModel(\"resnet50\",None,1000,resize=True,pretrained=False)\n",
    "resnet18 = ExperimentModel(\"resnet18\",None,1000,resize=True,pretrained=False)\n",
    "effnet = ExperimentModel(\"efficientnet_v2_m\",None,1000,resize=True,pretrained=True)\n",
    "mobilenet = ExperimentModel(\"mobilenet_v3_large\",None,1000,resize=True,pretrained=True)\n",
    "modelList = [resnet50,resnet18,effnet,mobilenet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "c:\\Users\\assaw\\.conda\\envs\\AI\\lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\amp.py:52: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model   | ResNet | 25.6 M | train\n",
      "1 | linear1 | Linear | 512 K  | train\n",
      "2 | linear2 | Linear | 16.4 K | train\n",
      "3 | linear3 | Linear | 99     | train\n",
      "-------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.344   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\assaw\\.conda\\envs\\AI\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\assaw\\.conda\\envs\\AI\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\assaw\\.conda\\envs\\AI\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 12/12 [00:08<00:00,  1.33it/s, v_num=1, train_loss_step=0.776]updated best score--------------------------------------------------------\n",
      "\n",
      " Validation accuracy: 0.3766666650772095\n",
      "debug: 5\n",
      "bestValScore: 0.3766666650772095\n",
      "Epoch 1: 100%|██████████| 12/12 [00:08<00:00,  1.40it/s, v_num=1, train_loss_step=0.600, val_loss=430.0, val_acc_F1=0.191, val_acc=0.191, train_loss_epoch=3.120]updated best score--------------------------------------------------------\n",
      "\n",
      " Validation accuracy: 0.6152777671813965\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 2: 100%|██████████| 12/12 [00:08<00:00,  1.44it/s, v_num=1, train_loss_step=0.743, val_loss=1.100, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=1.200]\n",
      " Validation accuracy: 0.6152777671813965\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 3: 100%|██████████| 12/12 [00:08<00:00,  1.46it/s, v_num=1, train_loss_step=0.788, val_loss=1.160, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=1.010]\n",
      " Validation accuracy: 0.4861111342906952\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 4: 100%|██████████| 12/12 [00:08<00:00,  1.44it/s, v_num=1, train_loss_step=0.641, val_loss=1.240, val_acc_F1=0.489, val_acc=0.489, train_loss_epoch=0.930]\n",
      " Validation accuracy: 0.6152777671813965\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 4: 100%|██████████| 12/12 [00:09<00:00,  1.23it/s, v_num=1, train_loss_step=0.641, val_loss=0.912, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=0.887]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 12/12 [00:11<00:00,  1.08it/s, v_num=1, train_loss_step=0.641, val_loss=0.912, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=0.887]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\assaw\\.conda\\envs\\AI\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:652: Checkpoint directory tb_logs\\my_model\\version_1\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model   | ResNet | 11.7 M | train\n",
      "1 | linear1 | Linear | 512 K  | train\n",
      "2 | linear2 | Linear | 16.4 K | train\n",
      "3 | linear3 | Linear | 99     | train\n",
      "-------------------------------------------\n",
      "12.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.2 M    Total params\n",
      "48.874    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 12/12 [00:08<00:00,  1.46it/s, v_num=1, train_loss_step=0.648]updated best score--------------------------------------------------------\n",
      "\n",
      " Validation accuracy: 0.4566666781902313\n",
      "debug: 5\n",
      "bestValScore: 0.4566666781902313\n",
      "Epoch 1: 100%|██████████| 12/12 [00:07<00:00,  1.55it/s, v_num=1, train_loss_step=0.565, val_loss=3.850, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=1.330]updated best score--------------------------------------------------------\n",
      "\n",
      " Validation accuracy: 0.6152777671813965\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 2: 100%|██████████| 12/12 [00:07<00:00,  1.51it/s, v_num=1, train_loss_step=0.580, val_loss=1.100, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=0.928]\n",
      " Validation accuracy: 0.6152777671813965\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 3: 100%|██████████| 12/12 [00:07<00:00,  1.55it/s, v_num=1, train_loss_step=0.643, val_loss=0.984, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=0.910]\n",
      " Validation accuracy: 0.6152777671813965\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 4: 100%|██████████| 12/12 [00:07<00:00,  1.50it/s, v_num=1, train_loss_step=0.606, val_loss=0.929, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=0.883]\n",
      " Validation accuracy: 0.5944444537162781\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 4: 100%|██████████| 12/12 [00:09<00:00,  1.29it/s, v_num=1, train_loss_step=0.606, val_loss=0.976, val_acc_F1=0.596, val_acc=0.596, train_loss_epoch=0.833]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 12/12 [00:09<00:00,  1.21it/s, v_num=1, train_loss_step=0.606, val_loss=0.976, val_acc_F1=0.596, val_acc=0.596, train_loss_epoch=0.833]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type         | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model   | EfficientNet | 54.1 M | train\n",
      "1 | linear1 | Linear       | 512 K  | train\n",
      "2 | linear2 | Linear       | 16.4 K | train\n",
      "3 | linear3 | Linear       | 99     | train\n",
      "-------------------------------------------------\n",
      "54.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "54.7 M    Total params\n",
      "218.674   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 12/12 [00:10<00:00,  1.16it/s, v_num=1, train_loss_step=1.030]updated best score--------------------------------------------------------\n",
      "\n",
      " Validation accuracy: 0.27916666865348816\n",
      "debug: 5\n",
      "bestValScore: 0.27916666865348816\n",
      "Epoch 1: 100%|██████████| 12/12 [00:11<00:00,  1.06it/s, v_num=1, train_loss_step=0.623, val_loss=1.170, val_acc_F1=0.277, val_acc=0.277, train_loss_epoch=1.340]updated best score--------------------------------------------------------\n",
      "\n",
      " Validation accuracy: 0.6152777671813965\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 2: 100%|██████████| 12/12 [00:10<00:00,  1.17it/s, v_num=1, train_loss_step=0.630, val_loss=0.965, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=0.957]\n",
      " Validation accuracy: 0.6152777671813965\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 3: 100%|██████████| 12/12 [00:10<00:00,  1.16it/s, v_num=1, train_loss_step=0.514, val_loss=0.940, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=0.938]\n",
      " Validation accuracy: 0.6152777671813965\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 4: 100%|██████████| 12/12 [00:10<00:00,  1.13it/s, v_num=1, train_loss_step=0.666, val_loss=0.885, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=0.850]\n",
      " Validation accuracy: 0.6152777671813965\n",
      "debug: 3\n",
      "bestValScore: 0.6152777671813965\n",
      "Epoch 4: 100%|██████████| 12/12 [00:12<00:00,  0.99it/s, v_num=1, train_loss_step=0.666, val_loss=0.911, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=0.899]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 12/12 [00:15<00:00,  0.78it/s, v_num=1, train_loss_step=0.666, val_loss=0.911, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=0.899]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type        | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model   | MobileNetV3 | 5.5 M  | train\n",
      "1 | linear1 | Linear      | 512 K  | train\n",
      "2 | linear2 | Linear      | 16.4 K | train\n",
      "3 | linear3 | Linear      | 99     | train\n",
      "------------------------------------------------\n",
      "6.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.0 M     Total params\n",
      "24.048    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 12/12 [00:08<00:00,  1.37it/s, v_num=1, train_loss_step=1.410]updated best score--------------------------------------------------------\n",
      "\n",
      " Validation accuracy: 0.5316666960716248\n",
      "debug: 5\n",
      "bestValScore: 0.5316666960716248\n",
      "Epoch 1: 100%|██████████| 12/12 [00:08<00:00,  1.42it/s, v_num=1, train_loss_step=1.360, val_loss=1.640, val_acc_F1=0.617, val_acc=0.617, train_loss_epoch=2.420]\n",
      " Validation accuracy: 0.3847222328186035\n",
      "debug: 3\n",
      "bestValScore: 0.5316666960716248\n",
      "Epoch 2: 100%|██████████| 12/12 [00:08<00:00,  1.38it/s, v_num=1, train_loss_step=1.290, val_loss=1.260, val_acc_F1=0.383, val_acc=0.383, train_loss_epoch=1.180]\n",
      " Validation accuracy: 0.4888889193534851\n",
      "debug: 3\n",
      "bestValScore: 0.5316666960716248\n",
      "Epoch 3: 100%|██████████| 12/12 [00:08<00:00,  1.49it/s, v_num=1, train_loss_step=0.0991, val_loss=1.230, val_acc_F1=0.489, val_acc=0.489, train_loss_epoch=1.260]updated best score--------------------------------------------------------\n",
      "\n",
      " Validation accuracy: 0.5500000715255737\n",
      "debug: 3\n",
      "bestValScore: 0.5500000715255737\n",
      "Epoch 4: 100%|██████████| 12/12 [00:08<00:00,  1.44it/s, v_num=1, train_loss_step=0.521, val_loss=2.530, val_acc_F1=0.553, val_acc=0.553, train_loss_epoch=0.717] \n",
      " Validation accuracy: 0.444444477558136\n",
      "debug: 3\n",
      "bestValScore: 0.5500000715255737\n",
      "Epoch 4: 100%|██████████| 12/12 [00:09<00:00,  1.22it/s, v_num=1, train_loss_step=0.521, val_loss=1.770, val_acc_F1=0.447, val_acc=0.447, train_loss_epoch=0.701]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 12/12 [00:10<00:00,  1.17it/s, v_num=1, train_loss_step=0.521, val_loss=1.770, val_acc_F1=0.447, val_acc=0.447, train_loss_epoch=0.701]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "# trainer.fit(model=alexNet,train_dataloaders=loader.trainLoader,val_dataloaders=loader.valLoader)\n",
    "for x in modelList:\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',  # Metric to monitor\n",
    "        patience=5,          # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=False,        # Verbosity mode\n",
    "        mode='min'           # Mode can be 'min', 'max', or 'auto'\n",
    "    )\n",
    "    trainer = L.Trainer(max_epochs = 5,accelerator='gpu', devices='auto', precision='16-mixed',callbacks=[early_stopping],logger=logger)\n",
    "    \n",
    "    trainer.fit(model=x,train_dataloaders=trainLoader,val_dataloaders=valLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ResNet\n",
      "bestValScore: 0.6152777671813965\n",
      "bestValPreds: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "model: ResNet\n",
      "bestValScore: 0.6152777671813965\n",
      "bestValPreds: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "model: EfficientNet\n",
      "bestValScore: 0.6152777671813965\n",
      "bestValPreds: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "model: MobileNetV3\n",
      "bestValScore: 0.5500000715255737\n",
      "bestValPreds: [0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 0, 0, 1, 0, 2, 0, 1, 1, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raise AssertionError(\"done\")\n",
    "for x in modelList:\n",
    "    print(f\"model: {x.model.__class__.__name__}\")\n",
    "    print(f\"bestValScore: {x.bestValScore}\")\n",
    "    print(f\"bestValPreds: {[y.argmax().item() for y in x.bestValPreds]}\")\n",
    "    # print(f\"bestValLabels: {x.bestValLabels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1])\n",
      "tensor([1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
      "        0, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 0, 0, 1, 0, 2, 0, 1, 1, 0])\n",
      "tensor([[0.2222, 0.7778, 0.0000],\n",
      "        [0.1724, 0.8276, 0.0000],\n",
      "        [0.1111, 0.8889, 0.0000]])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvkAAAJGCAYAAADBIJrYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNeUlEQVR4nO3de1yUdd7/8ffMAAN4QBA5SCoe8pxiKISdrCg7a0c7bBptdrdla3K3v6Itzbo3Oq3RbpbVZrUdrbayNldLysrEKEhL82yKJ06KoAgDzMzvD9qxcQZjTBmuq9fzflyPe/nO97quj7OrfHnzub5jcbvdbgEAAAAwDWuwCwAAAABwdLHIBwAAAEyGRT4AAABgMizyAQAAAJNhkQ8AAACYDIt8AAAAwGRY5AMAAAAmwyIfAAAAMJmQYBfwXw2PXxbsEgDDybTfFOwSAEPKmvJIsEsADCnLlR/sEvyaNP7lNrvXS+9d12b3+jVI8gEAAACTaTdJPgAAAHAkXFZLsEtod0jyAQAAAJMhyQcAAIChuUnyfZDkAwAAACbDIh8AAAAwGdp1AAAAYGguG+06hyLJBwAAAEyGJB8AAACGxhaavkjyAQAAAJMhyQcAAIChkeT7IskHAAAATIYkHwAAAIbGh2H5IskHAAAATIYkHwAAAIbGPvm+SPIBAAAAkyHJBwAAgKGxu44vknwAAADAZEjyAQAAYGguK7n1oXhHAAAAAJMhyQcAAIChsU++L5J8AAAAwGRI8gEAAGBo7JPviyQfAAAAMBkW+QAAAIDJ0K4DAAAAQ+PDsHyR5AMAAAAmQ5IPAAAAQ2MLTV8k+QAAAIDJkOQDAADA0OjJ90WSDwAAAJgMST4AAAAMjQ/D8kWSDwAAAJgMST4AAAAMjZ58XyT5AAAAgMmQ5AMAAMDQ2CffF0k+AAAAYDIk+QAAADA0evJ9keQDAAAAJkOSDwAAAEMjyfdFkg8AAACYDEk+AAAADI1PvPVFkg8AAACYDEk+AAAADI198n2R5AMAAAAmwyIfAAAAMBnadQAAAGBobKHpiyQfAAAAMBmSfAAAABgaSb4vknwAAADgGJo9e7aSk5MVHh6u9PR0FRYWHnZ+Xl6eBgwYoIiICPXo0UPTpk1TfX19QPckyQcAAIChudvxh2HNmzdP2dnZmjNnjtLT05WXl6exY8dq3bp1iouL85n/2muv6a677tLcuXM1evRorV+/Xtdff70sFotmzZrV6vuS5AMAAADHyKxZszR58mRlZWVp8ODBmjNnjiIjIzV37ly/85ctW6aTTz5Z11xzjZKTk3XOOefo6quv/sX0/1As8gEAAGBoLqulzQ6Hw6Gamhqvw+Fw+K2roaFBRUVFyszM9IxZrVZlZmaqoKDA7zmjR49WUVGRZ1G/efNmLViwQOeff35A7wmLfAAAAKCVcnNzFRUV5XXk5ub6nVtZWSmn06n4+Hiv8fj4eJWWlvo955prrtH999+vU045RaGhoerbt6/GjBmju+++O6A6WeQDAADA2KyWNjtycnJUXV3tdeTk5By1P8qSJUv04IMP6qmnnlJxcbHeeecdffjhh3rggQcCug4P3gIAAACtZLfbZbfbWzU3NjZWNptNZWVlXuNlZWVKSEjwe869996r6667TjfeeKMk6YQTTlBtba1uuukm/fnPf5bV2rqMniQfAAAAhma1utvsCERYWJhSU1OVn5/vGXO5XMrPz1dGRobfcw4cOOCzkLfZbJIkt7v19yfJBwAAAI6R7OxsTZo0SSNHjlRaWpry8vJUW1urrKwsSdLEiROVlJTk6eu/6KKLNGvWLI0YMULp6enauHGj7r33Xl100UWexX5rsMgHAACAoVltgSXsbWnChAmqqKjQ9OnTVVpaqpSUFC1cuNDzMG5JSYlXcn/PPffIYrHonnvu0Y4dO9StWzdddNFF+stf/hLQfS3uQHL/Y6jh8cuCXQJgOJn2m4JdAmBIWVMeCXYJgCFlufJ/eVIQnPT44ja71/Jpmb88qR0gyQcAAIChBdor/1vAg7cAAACAyZDkAwAAwNBI8n2R5AMAAAAmwyIfAAAAMBnadQAAAGBo7XkLzWAhyQcAAABMhiQfAAAAhsaDt75I8gEAAACTIckHAACAoZHk+yLJBwAAAEyGJB8AAACGRpLviyQfAAAAMBmSfAAAABga++T7IskHAAAATIYkHwAAAIZGT74vknwAAADAZEjyAQAAYGgk+b5I8gEAAACTIckHAACAoZHk+yLJBwAAAEyGJB8AAACGZmOffB8k+QAAAIDJkOQDAADA0OjJ98Ui30Csw8+VLXWc1KGL3BVb5Pz0ebnLNvqfOzRT1sGny9K1pyTJXb5ZzqWvHpxvtck2+mpZep8oS1S85DggV8l3ci59Raqtap7TuZts6VfI2mOo1KGLtL9KrrWfy/nVvyRXkyTJctwQ2U68UJaE46WwCLmrdslVNF+utV8c3VqAX+GSoT10VUpvxUSGadPufXrii7VaU17td+4T40ZpRFKMz3jB1grd+WGxJOnzW8b6PfepZev0xootSukerb+NT/M756a3C7S2vEZZo/oqa1Q/n9frGps09rn8VtcSEWLT/2T01ym94xQVHqpdNXV6+/uten/1dr/3B9qzgbeM09A7rlREQoyqVm7S8j/+XZVfrwt2WYAhscg3CGv/0bKddr2c+c/IVbpBthMvVMil96rxxdukuhqf+Zbjhsi1dqncu9bJ3dQo26jxCrl0uhr/ebtUu0cKscsS10eur96Wq2KLLPYOso25QSHj7lLTa3c2XyM6SbJY1LT4GbmrS2Xp2kMhmX+QQuxyfvHP5jmJA+Su2Crn1+/JfWCvrH1Gyjb2NrkdB+T+seio1QIcqTP7JejWkwfqr5+t1g9l1bpiWC89dmGqrn19qfbWNfjMv2fhCoVaLZ6vO4eHau6E0fp0Y6lnbPwLn3qdk94rVneeMVSfbS6TJK0q3esz5/fpxys1KUZry5v/vr7x7RbNX7XNa87j40Zp7c9++GhNLbeePEAnHtdV/7f4O5Xuq9OoHrGadtog7a516MstFa1+n4Bg633lGKX99WYt+0OeKr5aqyG3X6pzFj6sdwZer/qKvcEuDzAcevINwnriRXKtWizXD59Ke7bLufgZqckh69Cz/M53LnxCru8WyV2xRaraIefHT0sWi6w9T2ie0HBATe/cL9f6ZVLVTrlLN8j56T9kje8ndYqVJLm3rpDzo9lyl6yUqsvk3vyNnEXvy3r8SZ77uL5+R86CN+TetU6qLpPr2w/l3rJC1n7pR7UW4EhdObyX/v3Ddv1n7U5trarVXz/7QfVNTl0wMMnv/H2ORu2pa/Aco3rEytHk0pJNZZ45P399T12DTkmO07c79mhXTZ0kqcnl9nq92tGoU5K7acHaHZ5r1DU5veZER9rVO6ajPlyzPaBahiZ00cK1O7RiZ5VK99Xrgx+2a1PlPg2KizrabyVwTA2ZdrnW/2OBNr64SNVrtmrZzXlqOuDQ8TecG+zSYABWq7vNDqMIeJFfWVmpRx55RJdccokyMjKUkZGhSy65RI8++qgqKkiNjglriCzxfeUq+e5ng265Sr6TNbF/664REibZbHLX7295jr2D3G6X5KhtcYrFHil3/b7D38seKR3uPkepFuCXhFgt6t+ts77Zvtsz5pZUtH23hiR0adU1LhiUpPwNu1Tf5PT7enREmDJ6ddOHa3b4fV2STkmOU+fwMP1nbctzLhyUpJKqWn23a29Atawq3auTe8cptoNdkjSie4x6dOmgr7ftbukyQLtjDQ1R19T+2rm4+OCg261di4sVd9Lg4BUGGFhA7Tpff/21xo4dq8jISGVmZqp//+YFZllZmf72t7/poYce0qJFizRy5MjDXsfhcMjhcHiNWZqcsofYAiz/NyKikyxWm3Rgr/f4gWop2n8aeSjbqddJ+6vk9vpB4ecTQmU75XdyrV0qNdT5nxOVIGvKeXJ+/s8W72PtP1qW+H5y5j9zbGsBWiEqPEwhVquqDnj/e7OnrkE9ozv84vmD4qLUp2snPfzp6hbnnDuguw40OvX55rIW51wwKElfb6tURa3D7+thNqvO7t9drxZvDriWJ75Yoz+NGaJ3Jo1Rk9Mll6RHl6zWyl08zwLjsMdGyRpiU12Z9/9u68qrFDWwR5CqgpFY2ULTR0CL/Ntuu01XXHGF5syZI4vF4vWa2+3WzTffrNtuu00FBQWHvU5ubq5mzpzpNXbPOQN177n8tH4sWEddIuuAk9X01gzJ2ehngk0hF/yvJIucnzzr/yIdYhR66T1yrS+Qa9Viv1Msxw2V7Zxb5Vz8tNy7t/mdc1RqAdrIBYOStGn3vhYf0pWk8wcl6eP1O9XgdPl9vVsHu0b1iNV9H61s8Rqn9o5TZKhNC9ftDLiWy4b10uD4Lrrrw2KV7q9TSmKMpp06SJW19SravucX/oQAALMKqF1n5cqVmjZtms8CX5IsFoumTZumFStW/OJ1cnJyVF1d7XX8v8wBgZTy21K3T26XU4rs4j0eGeWb7h/CmnqxbCMvUdM7D8hdudXPhJ8W1Z27qemdmf6T8w7RCr1iplw718m5eI7f+1iSBitk3F1yfvaiXGs+O3a1AAGorm9Qk8ul6Ei713hMRJj2HPB96PbnwkNsOrNfgleP/KGGJXZRr+iO+vdhWnXOG5ikmvoGLd1S3uKcCwcfp2VbK1Tl50Hgw9USZrNqcvrxenLZWi3bWqHNu/frnVUl+mRjqa5K6X3YPx/Qnjgqq+VqcioiPtprPCIuWnWl/LCKX2a1tt1hFAGVmpCQoMLCwhZfLywsVHx8/C9ex263q3Pnzl4HrTqH4WqSu2yTrD1O+NmgRdYew+Tatb7F06wjx8mWfrma3n1A7rJNfib8tKjukqimf83030ffIUahV9wvV9lmOT+areaOZm+W44YoZPzdci59Ra7vPz52tQABanK5tb6iRqk/24bSIunE47pqdenew547pm+8Qm1WfbRuV4tzLhh0nNaWV2vT7pafUzl/YJIWrd8pp8v/r5ITO0VoRFLMYXv6W6olxGpRqM0q9yGXdrndsvpmMUC75Wps0u6i9Uo8a8TBQYtFiWeNUPnyH4JXGGBgAbXr3HHHHbrppptUVFSks846y7OgLysrU35+vp577jk99thjx6TQ3zpX8QfNW1OWb2reQnPEhVKoXa7Vn0iSbGNvk/bvkfPLVyVJ1pHjZcu4Sk3/yZO7puLgbwEa65sPq00hF94hS1wfNb33oGSxHpxTv795H/yfFvjufRVyfv6SFNH5YEE//QbBctxQhYzPkevbD+XasPzgNZxNkmP/0asFOEJvrtyqnDOHal1FjdaUN2+hGRFi8+x0c/dZQ1VZ69Czyzd4nXfBoOO09Mdy1Tj8tJVJigy1aUzfeM1e1vIe3icmxah7VKT+/UPLC/jzByVpd61DX5W0vHFBS7UcaHTq2x179IeM/nI0OVW2r17Du0dr7IDuevJL9haHsax+/G2d8uKd2v3NelUUrtWQ2y9TSIdwbXhhUbBLgwEYadebthLQIv/WW29VbGysHn/8cT311FNyOpt3eLDZbEpNTdWLL76oK6+88pgU+lvnWr9MioiSLeMq2SK7yF3xo5re/b/mh28lWTrFyv2zOM82bKwsIaEKvehPXtdxFsyTc/mbUscYWfs2f1hP6HWzvOY0vjVd7u2rZe01XJboRFmiExV203Necxoev0ySZB08RpbQcNnSLpMt7bKD9W5bpaa3Zxy1WoAj9cnGUnUJD9MNaf0UE2nXxsoa3fHvIk9rTHzHCJ8kvEeXSA3vHq3s979p8bpnHZ8oiyzK31Da4pwLBh2n73dVqWSv/12iLJLOG9hd/1m3Qy0E/b9Yy8yPVuqmk/rr3sxh6hweqtJ9dXruqw2av9r/czFAe/Xjm0sU3i1KI2Zer4iEaO1ZsUkfnXeX6st5iBw4Eha3+9Bvb63T2NioyspKSVJsbKxCQ0N/VSH/XTQCaL1M+03BLgEwpKwpjwS7BMCQslz5wS7Br2s/fLvN7vXqBZe32b1+jSP+xNvQ0FAlJiYezVoAAAAAHAVHvMgHAAAA2gP2yfdloI2AAAAAALQGST4AAAAMjd11fJHkAwAAACZDkg8AAABDI8n3RZIPAAAAmAxJPgAAAAyNJN8XST4AAABgMizyAQAAAJOhXQcAAACGxodh+SLJBwAAAEyGJB8AAACGxoO3vkjyAQAAAJMhyQcAAICh2SzBrqD9IckHAAAAjqHZs2crOTlZ4eHhSk9PV2FhYYtzx4wZI4vF4nNccMEFAd2TRT4AAAAMzWppuyNQ8+bNU3Z2tmbMmKHi4mINHz5cY8eOVXl5ud/577zzjnbt2uU5Vq1aJZvNpiuuuCKw9yTwUgEAAAC0xqxZszR58mRlZWVp8ODBmjNnjiIjIzV37ly/82NiYpSQkOA5Pv74Y0VGRga8yKcnHwAAAIbWlj35DodDDofDa8xut8tut/vMbWhoUFFRkXJycjxjVqtVmZmZKigoaNX9nn/+eV111VXq0KFDQHWS5AMAAACtlJubq6ioKK8jNzfX79zKyko5nU7Fx8d7jcfHx6u0tPQX71VYWKhVq1bpxhtvDLhOknwAAAAYWlsm+Tk5OcrOzvYa85fiHw3PP/+8TjjhBKWlpQV8Lot8AAAAoJVaas3xJzY2VjabTWVlZV7jZWVlSkhIOOy5tbW1euONN3T//fcfUZ206wAAAMDQbJa2OwIRFham1NRU5efne8ZcLpfy8/OVkZFx2HPfeustORwO/e53vzuSt4QkHwAAADhWsrOzNWnSJI0cOVJpaWnKy8tTbW2tsrKyJEkTJ05UUlKST1//888/r/Hjx6tr165HdF8W+QAAADC0I9m/vq1MmDBBFRUVmj59ukpLS5WSkqKFCxd6HsYtKSmR1erdXLNu3TotXbpUH3300RHfl0U+AAAAcAxNmTJFU6ZM8fvakiVLfMYGDBggt9v9q+7JIh8AAACG1pa76xgFD94CAAAAJsMiHwAAADAZ2nUAAABgaLTr+CLJBwAAAEyGJB8AAACGZiW29sFbAgAAAJgMST4AAAAMjZ58XyT5AAAAgMmQ5AMAAMDQSPJ9keQDAAAAJkOSDwAAAEOzkuT7IMkHAAAATIYkHwAAAIZms7iDXUK7Q5IPAAAAmAxJPgAAAAyN3XV8keQDAAAAJkOSDwAAAEMjyfdFkg8AAACYDEk+AAAADI198n2R5AMAAAAmQ5IPAAAAQ6Mn3xdJPgAAAGAyLPIBAAAAk6FdBwAAAIZGu44vknwAAADAZEjyAQAAYGhsoemLJB8AAAAwGZJ8AAAAGBo9+b5I8gEAAACTIckHAACAoZHk+yLJBwAAAEyGJB8AAACGRpLviyQfAAAAMBmSfAAAABga++T7IskHAAAATIYkHwAAAIZGT74vknwAAADAZEjyAQAAYGgk+b5I8gEAAACTIckHAACAobG7ji+SfAAAAMBkWOQDAAAAJkO7DgAAAAzNanEHu4R2hyQfAAAAMBmSfAAAABgaW2j6IskHAAAATIYkHwAAAIZGT74vknwAAADAZEjyAQAAYGh8GJYvknwAAADgGJo9e7aSk5MVHh6u9PR0FRYWHnb+3r17deuttyoxMVF2u139+/fXggULAronST4AAAAMzdaOe/LnzZun7OxszZkzR+np6crLy9PYsWO1bt06xcXF+cxvaGjQ2Wefrbi4OL399ttKSkrS1q1b1aVLl4DuyyIfAAAAaCWHwyGHw+E1ZrfbZbfb/c6fNWuWJk+erKysLEnSnDlz9OGHH2ru3Lm66667fObPnTtXe/bs0bJlyxQaGipJSk5ODrhO2nUAAABgaFZL2x25ubmKioryOnJzc/3W1dDQoKKiImVmZh6s1WpVZmamCgoK/J7z/vvvKyMjQ7feeqvi4+M1dOhQPfjgg3I6nQG9JyT5AAAAQCvl5OQoOzvba6ylFL+yslJOp1Px8fFe4/Hx8Vq7dq3fczZv3qxPPvlE1157rRYsWKCNGzfqlltuUWNjo2bMmNHqOlnkAwAAwNDacp/8w7XmHA0ul0txcXF69tlnZbPZlJqaqh07dujRRx9lkQ8AAAAEW2xsrGw2m8rKyrzGy8rKlJCQ4PecxMREhYaGymazecYGDRqk0tJSNTQ0KCwsrFX3picfAAAAhmaztN0RiLCwMKWmpio/P98z5nK5lJ+fr4yMDL/nnHzyydq4caNcLpdnbP369UpMTGz1Al9qR0m+Y0NNsEsADCekY1OwSwAAAIeRnZ2tSZMmaeTIkUpLS1NeXp5qa2s9u+1MnDhRSUlJnod3//CHP+jJJ5/U1KlTddttt2nDhg168MEH9cc//jGg+7abRT4AAABwJNrzJ95OmDBBFRUVmj59ukpLS5WSkqKFCxd6HsYtKSmR1XqwuaZHjx5atGiRpk2bpmHDhikpKUlTp07VnXfeGdB9WeQDAAAAx9CUKVM0ZcoUv68tWbLEZywjI0PLly//VfdkkQ8AAABDa8vddYyCB28BAAAAk2GRDwAAAJgM7ToAAAAwtEC3tvwtIMkHAAAATIYkHwAAAIbGg7e+SPIBAAAAkyHJBwAAgKG15w/DChaSfAAAAMBkSPIBAABgaDZ68n2Q5AMAAAAmQ5IPAAAAQ6Mn3xdJPgAAAGAyJPkAAAAwNPbJ90WSDwAAAJgMST4AAAAMjdTaF+8JAAAAYDIk+QAAADA09sn3RZIPAAAAmAxJPgAAAAyNffJ9keQDAAAAJsMiHwAAADAZ2nUAAABgaHwYli+SfAAAAMBkSPIBAABgaDYevPVBkg8AAACYDEk+AAAADI2efF8k+QAAAIDJkOQDAADA0PgwLF8k+QAAAIDJkOQDAADA0Gz05PsgyQcAAABMhiQfAAAAhkZPvi+SfAAAAMBkSPIBAABgaBZyax+8IwAAAIDJkOQDAADA0CwWmvIPRZIPAAAAmAxJPgAAAAyNnnxfvCMAAACAyZDkAwAAwNDoyfdFkg8AAACYDIt8AAAAwGRo1wEAAICh8eCtL94RAAAAwGRI8gEAAGBoFvHg7aFI8gEAAACTIckHAACAoVks5NaH4h0BAAAATIZFPgAAAAzN0ob/dyRmz56t5ORkhYeHKz09XYWFhS3OffHFF2WxWLyO8PDwgO/JIh8AAAA4RubNm6fs7GzNmDFDxcXFGj58uMaOHavy8vIWz+ncubN27drlObZu3RrwfVnkAwAAwNAsFmubHYGaNWuWJk+erKysLA0ePFhz5sxRZGSk5s6de5g/j0UJCQmeIz4+PuD7ssgHAAAAWsnhcKimpsbrcDgcfuc2NDSoqKhImZmZnjGr1arMzEwVFBS0eI/9+/erV69e6tGjh8aNG6fVq1cHXCeLfAAAABhaW/bk5+bmKioqyuvIzc31W1dlZaWcTqdPEh8fH6/S0lK/5wwYMEBz587V/Pnz9corr8jlcmn06NHavn17QO8JW2gCAAAArZSTk6Ps7GyvMbvdftSun5GRoYyMDM/Xo0eP1qBBg/TMM8/ogQceaPV1WOQDAADA0Cxt2Jxit9tbvaiPjY2VzWZTWVmZ13hZWZkSEhJadY3Q0FCNGDFCGzduDKhO2nUAAACAYyAsLEypqanKz8/3jLlcLuXn53ul9YfjdDr1/fffKzExMaB7k+QDAADA0CyWI9u/vi1kZ2dr0qRJGjlypNLS0pSXl6fa2lplZWVJkiZOnKikpCRPX//999+vk046Sf369dPevXv16KOPauvWrbrxxhsDui+LfAAAAOAYmTBhgioqKjR9+nSVlpYqJSVFCxcu9DyMW1JSIqv1YHNNVVWVJk+erNLSUkVHRys1NVXLli3T4MGDA7qvxe12u4/qn+QI7bvl7GCXABjOuI63B7sEwJCue2xWsEsADCnLlf/Lk4Jgf+O/2uxeHUMva7N7/Rr05AMAAAAmwyIfAAAAMBl68gEAAGBoFrXfB2+DhSQfAAAAMBmSfAAAABiaxUJufSjeEQAAAMBkSPIBAABgaPTk+yLJBwAAAEyGJB8AAACGRk++L94RAAAAwGRI8gEAAGBoFnJrH7wjAAAAgMmQ5AMAAMDQ2F3HF0k+AAAAYDIk+QAAADA0dtfxxTsCAAAAmAxJPgAAAAyNnnxfJPkAAACAyZDkAwAAwNDoyffFOwIAAACYDEk+AAAADI2efF8k+QAAAIDJkOQbSOhpFyvs7Ctk6Rwj1/ZNqn9ztlxb1/mda03spbALJ8nW83hZuyao/q2n1Pjpu15zOjzwsqxdE3zObfjsfTnm/V2K7CT7hRMVMihVlug4ufdXq2nll3J88KJUf8D3ph06qcPdz8ga3U37/ne8VFcrSbJ0jpH9sv+RrWd/Wbp1V+OS9+R4++kW/5whqWMU8fs/q3Hll6p/5r7Wvj1Ai8Zl9NKE0/soppNdm3bV6O/zV2vttuoW5192SrIuzuiluC4Rqq5t0Off79Jz/1mnxiaXJOnqM/rq1KEJ6hnXUY5Gp1ZvqdJz/1mrbRXN/5uPj47Q6zln+r32zJeL9Nn3pZKkTx65wOf1B14t1qcrd3m+PmtEd111el8lxXZQbX2jCtdV6JkP16jmQKNnTofwEP3+3AE6dWiCOkWGqqyqTk998IO+WlsR+JsFBNHAW8Zp6B1XKiIhRlUrN2n5H/+uyq/9f58DcHgs8g0iJPV02S/7H9W//je5tqxR6JmXKvK2XNXed4Pc+/f6nhBml7tylxzFn8t++c1+r3ng4SmS9eAvc6yJyYqc+oiaij9r/jqqqyxRXVX/zrNy7doqa0y8wq+e2jz2jwd8rhf+u/+Va8ePskZ3O6T4ULn3V8ux8FWFnXnZYf+clph42S+9SU0bvjv8GwK00pjhifrDRYOU984qrSnZq8tO7a2Hf5+uSY8u0d7aBp/5Z6Z01+TzBuqRt77T6q1V6hHbQf9vwnC53dLT/14jSRreJ0bzl23Vuu17ZbVadOO5A/XIjWnKeuxz1Tc6VbG3Tpfdv9jruhee1EMTTu+rr9Z5L7wfnrdShT8b219/cPE+pFe07pqQoqc++EEFP5QpNipc0y49Qf972TDNeLlIkhRis+jRyenau79B971crMqaesVHR2h/XaMAI+l95Ril/fVmLftDniq+Wqsht1+qcxY+rHcGXq/6ir3BLg/tnIXmFB+8IwYRduZlavzyP2pavkiu0hI5Xn9C7gaHQkeP9TvftXW9HO8+p6aiJVKT/2/27v3VctdUeY6QE06Sq3yHnD8tsF27tqj+ufvl/H653JW75Fy/Qo73X1DICSd5/XAgSaGnXihLREc1LH7L9z57yuR46yk1fbVY7p/Sfb8sVkVk5ajhw3/KXVnaujcG+AVXnNpbC77apoXfbNfW8v16/J3v5Wh06rxRPfzOH9orWqu2VOmTFTtVVlWnbzZU6pMVOzWwRxfPnLue/1qLirZrS9l+bd61Tw+/uVLx0ZHqf1yUJMnllqr2O7yOU4YkaMnKXapvcHrdb399o9e8//62QGpe5JdVHdC7X25RaVWdVm2p0r+Xl2hgjyjPnPNG9VDnyFDd+9I3Wr21SmVVdfpu8x5t3rXvKL6LwLE3ZNrlWv+PBdr44iJVr9mqZTfnqemAQ8ffcG6wSwMMiUW+EdhCZO3ZX851xQfH3G451xbL2nvwUbtHSNpZaixYdNhplogOctcfkFwHFyLWhJ4KO/93qn/pYcntOszZhxd2/u/k2lelxmULj/gawM+F2CzqnxSloo2VnjG3WyraUKnBvbr4PWfV1ir1Py7Ks5BOjIlQ+oA4fbW2vMX7dAhv/qVozQHf3wxI0vFJnXV8UpT+8/U2n9emjh+qd2ecraemnKxzRx7n9drqrVXqFhWh9IHNvx2L7him04YleP02YPTgeK3euldTLxmqt+/N1PPZp+maM/rKyjNoMBBraIi6pvbXzsXe3+d2LS5W3ElH6fscTM1isbTZYRRHvV1n27ZtmjFjhubOndviHIfDIYfD4TXW4HTJbuNnDn8sHaNksdnkqqnyGnfvq5It3n8aGaiQ4aNlieioxuUftVxHh84KO+9aNX654Gcnhir8hrvlePc5uasqpNjEI7q/re8QhY4+Vwce9N9aBByJqA5hstmsqtrn/e9N1X6HesZ18HvOJyt2KqpDmJ74w2hZLFKIzar3C7bqtU83+Z1vsUi3XjxY3/+4R1vK9vudc/6ontpStk+rt3r/HZ67aJ2+3bhbjkanRvaP1e2XDFWEPUTvfrlFUvMi/8HXv9W9156osBCrQmxWLfuhTE+8u8pzjcSYSI3oG6HF3+5UztxCJcV20NTxQxVis+qfize09q0CgsoeGyVriE11Zd5/R+rKqxQ18Oh8nwN+a476qnrPnj166aWXDjsnNzdXUVFRXsdfi3882qUgAKGjz5Pzh0K5q3f7nxAeqYhb/k+u0q1q+Pc/PcP2cTfIVVqipsL8I7+5PULhk+5U/auPy11bc+TXAY6C4X1idO2ZffXEe6v0P08s1fSXvlH6wDj97qx+fudPHT9UveM76YHXvvX7eliIVWeN6O43xX8lf6NWb63Sxp01emPJZr3x2WZNOL2P5/VecR1167ghennxBt38xFL9v398pfjoCE279ATPHItFqtrfoFn/+k4bdtRoycpdevWTjbropJ6/8p0AAANxt+FhEAEn+e+///5hX9+8efMvXiMnJ0fZ2dleYw1/uiTQUn4z3Pur5XY6Ze0crZ83w1g6Rfuk+0fCEhMn28ARqn92pv8J9ghFTnlQbked6p65T3Id7Cm29R8ha1KyOo447aeLNf+/jo/8Sw0LX1PDh//0vd4hrN26yxqbqIg//Oxh3p9+Hdbx7wtVOzNL7spdLZwNtKy6tkFOp0vRnexe49Ed7dpzSLr/X1ljB+jj4h1aUNi8KP+xdJ/Cw0KUfdkJevWTjXL/7B/4P44bopMGxen2pwtUWV3v93qnD0uUPdSmj4p2/GK9a0r2amLm8Qq1WdXodOmaM/pq9ZYqzfus+d/VzaX7VN+wSn+7ZbTmLlqnPfsc2rPPoSanW66f1VVSvl9dO4crxGZRk9NA35Hwm+WorJaryamI+Giv8Yi4aNWV7glSVYCxBbzIHz9+vCwWi9zulr9x/FK/kt1ul93u/U13H606LXM2yVWyXrYBI9S0clnzmMUi24ARavxs/q++fGjGWLn37VXTqq98XwyPVOSUXLmbGlX39HSfh3jrnpspS+jB/y6tvQYoYuIdOjBrmtwVrVuYu0pLVPvAZK+xsIuvl8UeKcdbTzW3AQFHoMnp1vod1TqxX6y+XF0mqfnnxxP7ddV7y7b6PSc81PbzR04kSa6f/r2z6GCI88dxQ3TK0ARNe6ZApVV1LdZw3qgeWvZDmar97ORzqH7dO6vmQIManc0F2MNscrq8/611/fT1f/+ZXbWlSmeldJfFIs8PIMfFdlBlTT0LfBiGq7FJu4vWK/GsESqZ/2XzoMWixLNGaM3s94JaGwziVzwTGDCDtOUHvLJOTEzUO++8I5fL5fcoLi7+5YsgYA2f/EuhJ5+vkPSzZU3oKftVf5TFHu55UDZ80v9T2LgbDp5gC5H1uL6yHtdXsoXK2iVW1uP6ytKtu/eFLRaFnjRWjcs/ls/KJjxSkbc9JNnDVf/KX2WJiJSlc7QsnaMlS/P/dNyVu+TatcVzuHc3L+xdpSVeW3v+txaLPUKWjlHNXyf81E7Q1Oh1DdeuLdKBWrkdB5r/s7PpKL6T+K1564sfdUFaD52TmqSecR11+yVDFR4WooXfNCf1d00YrhvPHeCZX7CmTBdn9NQZwxOVEB2h1ONjlXVOfxWsKfOk5VPHD1XmiUn6v9e/1YF6p6I72hXd0a6wEO9/Urt3jdSw3jGe3wr8XMagOJ2f1kPJ8R3VvWukLj6pp645s6+nH1+SCn4o16lDE3TxST2VGBOhIb2iNWXcEK0pqdLumubfRLxfsFWdIkM15eIhOi62g9IHxumaM/tpfgs/xADt1erH31b/Gy9Qv4nnKGpgT41++naFdAjXhhcOvyEEAP8CTvJTU1NVVFSkcePG+X39l1J+HJmmos/k6NhF9gsnydI5Wq7tm3Tgybvl3rdXkmSJjpP1Z4mfJaqrOtw9x/N12NlXKuzsK9W0fqXq8u7wjNsGnihr13g1FvjuaGPr0U+23oMkSR3v92672X/P7+TeU9bq+n9ei61Xf4WmnSXX7lLV3ntdq68BHIklK3epS4cwZZ3TX9Gd7Nq0s0Z3Pl+oqv3NyXpclwhPUi9JL+c3t+TcMHaAYqPCtXd/gwrWlOn5hQc/kGfc6F6SpLybM7zu9fC8lVpUtN3z9Xmjeqiiul7fbPD9bVST061xGb10y0WDZZG0Y3etnv5gjT4sLPHMWVS0XZH2EI0fnaybLxys/fWN+nbjbj23YI1nTkV1ve78R6FuuWiw/jHtVFXW1OudpT/qjSX+HxQG2qsf31yi8G5RGjHzekUkRGvPik366Ly7VF/+69tS8RvQlkm+QVjcAa7Iv/jiC9XW1urcc/3vW1tbW6tvvvlGp59+ekCF7Lvl7IDmA5DGdbw92CUAhnTdY7OCXQJgSFmuX7HRxrHk/Ljt7mUzxpo14CT/1FNPPezrHTp0CHiBDwAAABwxknwfPO0KAAAAmMxR/zAsAAAAoE2R5PsgyQcAAABMhiQfAAAAxnboNuAgyQcAAADMhiQfAAAAxkZPvg+SfAAAAMBkWOQDAAAAJkO7DgAAAIyNdh0fJPkAAACAyZDkAwAAwNhI8n2Q5AMAAAAmQ5IPAAAAY+PDsHyQ5AMAAAAmQ5IPAAAAY6Mn3wdJPgAAAGAyLPIBAABgbG5X2x1HYPbs2UpOTlZ4eLjS09NVWFjYqvPeeOMNWSwWjR8/PuB7ssgHAAAAjpF58+YpOztbM2bMUHFxsYYPH66xY8eqvLz8sOdt2bJFd9xxh0499dQjui+LfAAAABhbO07yZ82apcmTJysrK0uDBw/WnDlzFBkZqblz57Z4jtPp1LXXXquZM2eqT58+R/SWsMgHAAAAWsnhcKimpsbrcDgcfuc2NDSoqKhImZmZnjGr1arMzEwVFBS0eI/7779fcXFx+v3vf3/EdbLIBwAAgKG53c42O3JzcxUVFeV15Obm+q2rsrJSTqdT8fHxXuPx8fEqLS31e87SpUv1/PPP67nnnvtV7wlbaAIAAACtlJOTo+zsbK8xu91+VK69b98+XXfddXruuecUGxv7q67FIh8AAADG1oafeGu321u9qI+NjZXNZlNZWZnXeFlZmRISEnzmb9q0SVu2bNFFF13kGXP99GcLCQnRunXr1Ldv31bdm3YdAAAA4BgICwtTamqq8vPzPWMul0v5+fnKyMjwmT9w4EB9//33WrFihee4+OKLdcYZZ2jFihXq0aNHq+9Nkg8AAABja8efeJudna1JkyZp5MiRSktLU15enmpra5WVlSVJmjhxopKSkpSbm6vw8HANHTrU6/wuXbpIks/4L2GRDwAAABwjEyZMUEVFhaZPn67S0lKlpKRo4cKFnodxS0pKZLUe/eYai9vtdh/1qx6BfbecHewSAMMZ1/H2YJcAGNJ1j80KdgmAIWW58n95UhC4K1vec/5os8Te0Gb3+jXoyQcAAABMhkU+AAAAYDL05AMAAMDY2vGDt8FCkg8AAACYDEk+AAAAjI0k3wdJPgAAAGAyJPkAAAAwNhdJ/qFI8gEAAACTIckHAACAsdGT74MkHwAAADAZknwAAAAYG0m+D5J8AAAAwGRI8gEAAGBsJPk+SPIBAAAAkyHJBwAAgLGxT74PknwAAADAZEjyAQAAYGz05PsgyQcAAABMhiQfAAAAxkaS74MkHwAAADAZFvkAAACAydCuAwAAAGNjC00fJPkAAACAyZDkAwAAwNhc7mBX0O6Q5AMAAAAmQ5IPAAAAY6Mn3wdJPgAAAGAyJPkAAAAwNpJ8HyT5AAAAgMmQ5AMAAMDY2F3HB0k+AAAAYDIk+QAAADA2evJ9kOQDAAAAJkOSDwAAAGMjyfdBkg8AAACYDEk+AAAAjI3ddXyQ5AMAAAAmQ5IPAAAAY6Mn3wdJPgAAAGAyJPkAAAAwNnryfZDkAwAAACbDIh8AAAAwGdp1AAAAYGw8eOuDJB8AAAAwGZJ8AAAAGBtJvg+SfAAAAMBkSPIBAABgaG53222haWmzO/06JPkAAACAyZDkAwAAwNjoyfdBkg8AAACYDIt8AAAAGJvL1XbHEZg9e7aSk5MVHh6u9PR0FRYWtjj3nXfe0ciRI9WlSxd16NBBKSkpevnllwO+J4t8AAAA4BiZN2+esrOzNWPGDBUXF2v48OEaO3asysvL/c6PiYnRn//8ZxUUFOi7775TVlaWsrKytGjRooDuyyIfAAAAxuZyt9nhcDhUU1PjdTgcjhZLmzVrliZPnqysrCwNHjxYc+bMUWRkpObOnet3/pgxY3TJJZdo0KBB6tu3r6ZOnaphw4Zp6dKlAb0lLPIBAACAVsrNzVVUVJTXkZub63duQ0ODioqKlJmZ6RmzWq3KzMxUQUHBL97L7XYrPz9f69at02mnnRZQneyuAwAAAGNrw911cnJylJ2d7TVmt9v9zq2srJTT6VR8fLzXeHx8vNauXdviPaqrq5WUlCSHwyGbzaannnpKZ599dkB1ssgHAAAAWslut7e4qD9aOnXqpBUrVmj//v3Kz89Xdna2+vTpozFjxrT6Gu1mkV+xNTzYJQCG0yN0T7BLAAAg+NrpPvmxsbGy2WwqKyvzGi8rK1NCQkKL51mtVvXr10+SlJKSojVr1ig3NzegRT49+QAAAMAxEBYWptTUVOXn53vGXC6X8vPzlZGR0erruFyuwz7c60+7SfIBAACAI+JyB7uCFmVnZ2vSpEkaOXKk0tLSlJeXp9raWmVlZUmSJk6cqKSkJM/Du7m5uRo5cqT69u0rh8OhBQsW6OWXX9bTTz8d0H1Z5AMAAADHyIQJE1RRUaHp06ertLRUKSkpWrhwoedh3JKSElmtB5tramtrdcstt2j79u2KiIjQwIED9corr2jChAkB3dfidrvbxY8+my+4KNglAIYzM/TKYJcAGNKY918MdgmAIWW58n95UhC4Ft/WZveyZv69ze71a5DkAwAAwNja6YO3wcSDtwAAAIDJkOQDAADA2EjyfZDkAwAAACZDkg8AAABja8dbaAYLST4AAABgMiT5AAAAMDZ68n2Q5AMAAAAmQ5IPAAAAYyPJ90GSDwAAAJgMST4AAACMjd11fJDkAwAAACZDkg8AAABjoyffB0k+AAAAYDIk+QAAADA0t5Oe/EOR5AMAAAAmQ5IPAAAAY2N3HR8k+QAAAIDJkOQDAADA2OjJ90GSDwAAAJgMi3wAAADAZGjXAQAAgKG5efDWB0k+AAAAYDIk+QAAADA2Hrz1QZIPAAAAmAxJPgAAAIzN6Qp2Be0OST4AAABgMiT5AAAAMDR21/FFkg8AAACYDEk+AAAAjI3ddXyQ5AMAAAAmQ5IPAAAAY6Mn3wdJPgAAAGAyJPkAAAAwNDc9+T5I8gEAAACTIckHAACAsbn4xNtDkeQDAAAAJkOSDwAAAGOjJ98HST4AAABgMizyAQAAAJOhXQcAAACG5ubDsHyQ5AMAAAAmQ5IPAAAAY+PBWx8k+QAAAIDJkOQDAADA2EjyfZDkAwAAACZDkg8AAABDY3cdXyT5AAAAgMmQ5AMAAMDYnK5gV9DukOQDAAAAx9Ds2bOVnJys8PBwpaenq7CwsMW5zz33nE499VRFR0crOjpamZmZh53fEhb5AAAAMDS3y91mR6DmzZun7OxszZgxQ8XFxRo+fLjGjh2r8vJyv/OXLFmiq6++Wp9++qkKCgrUo0cPnXPOOdqxY0dA92WRDwAAABwjs2bN0uTJk5WVlaXBgwdrzpw5ioyM1Ny5c/3Of/XVV3XLLbcoJSVFAwcO1D/+8Q+5XC7l5+cHdF968gEAAGBsbbhPvsPhkMPh8Bqz2+2y2+0+cxsaGlRUVKScnBzPmNVqVWZmpgoKClp1vwMHDqixsVExMTEB1UmSDwAAALRSbm6uoqKivI7c3Fy/cysrK+V0OhUfH+81Hh8fr9LS0lbd784771T37t2VmZkZUJ0k+QAAADC2NtwnP+eeHGVnZ3uN+Uvxj4aHHnpIb7zxhpYsWaLw8PCAzmWRDwAAALRSS605/sTGxspms6msrMxrvKysTAkJCYc997HHHtNDDz2kxYsXa9iwYQHXSbsOAAAADM3tdLfZEYiwsDClpqZ6PTT734doMzIyWjzvkUce0QMPPKCFCxdq5MiRR/SekOQDAAAAx0h2drYmTZqkkSNHKi0tTXl5eaqtrVVWVpYkaeLEiUpKSvL09T/88MOaPn26XnvtNSUnJ3t69zt27KiOHTu2+r4s8gEAAGBsbdiTH6gJEyaooqJC06dPV2lpqVJSUrRw4ULPw7glJSWyWg821zz99NNqaGjQ5Zdf7nWdGTNm6L777mv1fVnkAwAAAMfQlClTNGXKFL+vLVmyxOvrLVu2HJV70pMPAAAAmAxJPgAAAIzN6Qp2Be0OST4AAABgMiT5AAAAMDR3O37wNlhI8gEAAACTIckHAACAsQX4IVW/BST5AAAAgMmQ5AMAAMDQ3Gyu44MkHwAAADAZknwAAAAYmttlCXYJ7Q5JPgAAAGAyJPkAAAAwNBc9+T5I8gEAAACTIckHAACAobnd9OQfiiQfAAAAMBmSfAAAABga++T7IskHAAAATIYkHwAAAIbGPvm+SPIBAAAAkyHJN5DOF5yvqMsulS06Wg0//qjdc56RY/0Gv3NDe/ZUzO+uVVi/vgqNj1fls8+pZv77XnPChwxR1GWXyt6vr0K6dlXpA3/RgeXLveZEjs5Q5/POk71fX9k6d9b22/6ohs0/es3pdO5YdTz9dNn79ZU1MlJbrrxKrtparzldJlypyFEjFda7j9xNjdo64Wqfmrv+z00KHzxIYb16qWHbNu24beqRvE2Al7PO66/zLhmiqC4R2ralSq88V6jNG3a3OP+ciwbqzHP7q2tsB+3b59A3y0r01svFamxsbvi0WC265KphGn16H0V1Cdfeqjp98ckmvf/m955r2MNDdOV1I3Rieg917GRXRfl+ffzvtfp00cG/r1FdwjXh+lQNGZ6oiIhQ7dpRrQ/eXqVvCko8c+K7d9JVk1J1/KBuCgmxatuWvfrXayu0dlWZZ87gYQm69JrhOq5XtBz1Tfry0016+5UVcrncR/NtBNrEwFvGaegdVyoiIUZVKzdp+R//rsqv1wW7LMCQSPINosOpp6jr5BtV9drr2vHH29Xw449KeOB+WaOi/M632u1qLC3VnhdfUtOePX7nWMLD1fDjj6p8ek6L97Xaw1X/ww/a88JLLc6x2O06UFysqjffanlOSIhql36pmgULWpwjSfs++lj7P//isHOA1ko7uZeuvmGk5r/xnWZkf6htW6p0x4yz1Ckq3O/8k05L1hXXnaj35n2nnNve19wnC5R2Si9d/rsRnjkXXDpEZ57bXy8/W6ic297XvJeKdf4lQ3T2BQM9c665YaROOLG7nsn7Ujm3va+PPlir625K04hRx3nm3HT7yUrs3llPPPip/jz1AxUt36Zb7zhVPXtHe+Zk//lMWW0WPXzvx5rxvwtUsqVK2fecqaguzfX3SI5W9r1n6vvinZo+7UM99djnGjGqh66ceLBewCh6XzlGaX+9WSvu/6feT71Ze77bpHMWPqzwbl2CXRoMwOVqu8MoWOQbRNQl41WzcJH2L85X47ZtqnzyKbnrHep0ztl+5zs2bNCeuS+o9vMv5G5s9DunrqhIVS+/ogMFy/2+Lkn7P/1Ue19/Q3UrVrQ4p2b++6p+62051q5tcU7Vq6+p+r35ati6tcU5u595VjUfLlBTaWmLc4BAnDtusD77aIO++GSTdm6v1otPL1eDw6nTzurrd/7xA7ppw9pyLf98iyrLa7VqxS4t/2KL+hwf6zWnuHC7VhbtUGV5rb4pKNGqFTvV5/iunjn9BnTT0k83a+2qMlWW12rJRxu0bUuV13X6Deimjxes1eYNu1VRtl/vv/W9DtQ2qnff5ut07GRXQlJnffjOam3buldlu/bprX8Wyx4eoqSeXSRJ6af00rYtVZr/5vcqL92ndavLNe+fxTrrvAEKD+cXtTCWIdMu1/p/LNDGFxepes1WLbs5T00HHDr+hnODXRpgSCzyjSAkRPZ+/VS3YuXBMbdbdStWKHzggODVBbRjthCrkvvGaPV3B39odLul1St3qd+Abn7P2bCuQsl9u3oW7N3iO2r4iUlaWbzDa87gYQmK795JUnOa3n9QnL4r3umZs3FdhUaMOk7RMRGSpIFD4xXfvbNWrfCek35ysjp0DJPFIqWfkqzQMJvW/NSKs3+fQzu3V+vkMX0UZg+R1WrRGWP7q3pvnbZsav7tXEioTY2NTq8/Q4OjSWH2ECX36yrAKKyhIeqa2l87FxcfHHS7tWtxseJOGhy8wmAYbpelzQ6jCErU43A45HA4vMecTtlttmCU0+7ZOneWxWaTc2+V17hz716F9jiuhbOA37ZOneyy2ayq3lvnNV5dXa/E4/y3uS3/fIs6dQrXnx8cK1ksCgmx6pP/rNO/317lmfPhv1YpIiJUDz05Ti6XW1arRf96dYUKPj/4rMrLzxYq65aTlDf3cjU1ueR2u/XC7OVa90O5Z87sRz/XLXecpqdemaCmJpcaHE3620NLVF66zzPnkRmLNTVnjJ55/Sq53W7VVNfrsZn5OlDbIEla9e1Ojb1woE46NVlffblVXbqEa/yEYZKkLtERv/5NBNqIPTZK1hCb6sq8v8/VlVcpamCPIFUFGFvAi/y6ujoVFRUpJiZGgwd7/3RdX1+vN998UxMnTjzsNXJzczVz5kyvsT/2O15T+5NKAwiegUPjdeHlQ/XPZwq1aUOl4hM66dobR+niqjrPg7VpJycr4/TemjNrqXZs26uevaN17Q2jVLXngL78dLMk6ewLBqrvgFg9/pdPtbt8vwYMidd1/5Omqj0H9MNPv1m49JoURXYI08PTP9a+GodS03volj+dpgfvXqTtW/dKkibelKaa6no9ePciNTQ4dfrZ/TTtz2fovj/9R9VVdVq1YpfeeKlYk25O1023n6ymRpfmv/mdBgyJ58FbAL8pfBiWr4DaddavX69BgwbptNNO0wknnKDTTz9du3bt8rxeXV2trKysX7xOTk6OqqurvY6b+/YLvPrfCGdNjdxOp2xdor3GbV26yFlV1cJZwG/bvn0OOZ0uRXXxTrSjosJVXVXn95xLrxmuZUs267PFG7V9614VfbVNb7/yrS68bKgsP/2GdsL1J+rDf63SV0u3aPvWvVq25Ect+mCNLrxsqCQpNMymy3+XotfnFmnF19u1beteLV6wToVLt+i88c3BSFxCR519wUA9//dl+uG7Um3bUqX35n2nLRt366zzmsOOwcMSlDIySU899oU2rK3Q1s179M9nCtXQ4NQpZ/Tx1Lzo/TX6w7XzlH3jO7p14psqLtwmSaoo239U30/gWHJUVsvV5FREvPf3uYi4aNWV+t88AsDhBbTIv/POOzV06FCVl5dr3bp16tSpk04++WSVlJT88sk/Y7fb1blzZ6+DVp3DaGqSY+NGRaQMOzhmsSgiZbjq17K1GOCPs8mlLZv2aPCwBM+YxdK8eN64rsLvOXZ7iNxu7wTck4j/tMq3h4XokCnNbTs/vW6zWRUSavN7Hau1eU6YvfmXqK5WzDn0Om63Wxarb0/o3qo6NTY4ddKpvbW7olZbNrMwgnG4Gpu0u2i9Es/62c5QFosSzxqh8uU/BK8wGIbbbWmzwygCatdZtmyZFi9erNjYWMXGxuqDDz7QLbfcolNPPVWffvqpOnTocKzq/M2rfvc9dcueJseGjXKsX6+oceNkCQ/X/o8XS5K6ZU9T0+7dqnrpn80nhIQorGdzH6MlJEQhXbsqrE9vuerq1fTTb18s4eEK7Z7ouUdoQrzC+vSWc99+OSuaF0HWjh0VEtdNtpiY5jlJSZIkZ1WVnFV7JUm26C6yRUcrNLG7JCksuZdcdXVqKq+Qa39zmmjr1k22Th0V0q2bLFarwvr0liQ17twld319c8mJibJGhMsWHS1LWJhnTkPJNqmp6ei/qTC9hfN/0OSpJ+vHjbu1eUOlxl40SPbwEH2Rv0mSdNPU0araXae3XvlWkvTt19t17sWDtHVzlTatr1R8Yiddes1wrfh6u9w/Lfa//Wa7Lrp8qHZX1GrHtr3q1TtGYy8epC/yN0qS6usatWZVqSZMSlVDg1OV5bUaODROJ4/po9dfKJIk7dperdKdNcr6w0l648Ui7d/n0InpPTRkeKIe/8snkqSNaytUW9ugyVNHa/6879XQ0KQxZx+vbnEdtfKbgw8Cnzd+sL7/dqfcLrdSM3rqwkuHaPZjX3jqBYxi9eNv65QX79Tub9aronCthtx+mUI6hGvDC4uCXRpgSBb3oTHRYXTu3FlfffWVBg0a5DU+ZcoUzZ8/X6+99prGjBkjp9PZwhVatvmCiwI+57em84UXKOqySxUSHS3H5s3a/cyzcqxbL0lKzH1QTeXlqng8T5IUEhenni8873ONuu++166cuyVJ4ScMVfeHcn3m7Fuc77lOx8yzFDftdp85Va++pqrXXpckRV9ztaKvvcZnTvnjedq/OF+S1G3a7eqUeZbPnJ135aj++1WeP0PEsBN85pRk/V5N5eU+45Bmhl4Z7BLavczzB+i88YMVFR2hkh+r9MpzX2vzhkpJ0l3/d7Yqy2v1j78tkyRZrRZdfMUJGj2mt6JjIrWvxqFvv96uf736rQ7UNm9FGx4eokuvTVFqeg91jmr+MKzln2/Re29+J2dTc1NoVJdwXXHdCA1N6a4OHcNUWdG8jeai99d46opP7KQrJo5Q/0FxCg8PVdmuGv1n/g9atuTgA7zJfWN0+e9GqHffrrKFWLSjpFrz3/zOayefO+8/W736xig0xKqSLVWaP8/7dfg35v0Xg10C/Bh06zgNvWOCIhKitWfFJi2f+qQqC1venhltL8uVH+wS/Cq95rw2u1fCa/9ps3v9GgEt8tPS0nTbbbfpuuuu83ltypQpevXVV1VTU8MiH2gjLPKBI8MiHzgyLPKNs8gPqCf/kksu0euvv+73tSeffFJXX321T/8oAAAAcCy5XJY2O4wioEV+Tk6OFixY0OLrTz31lFxG+rxfAAAAwIT43HMAAAAYGvvk+wooyQcAAADQ/pHkAwAAwNDcBuqVbysk+QAAAIDJkOQDAADA0OjJ90WSDwAAAJgMST4AAAAMze2mJ/9QJPkAAACAybDIBwAAAEyGdh0AAAAYmosHb32Q5AMAAAAmQ5IPAAAAQ2MLTV8k+QAAAIDJkOQDAADA0NwuttA8FEk+AAAAYDIk+QAAADA0evJ9keQDAAAAJkOSDwAAAENz0ZPvgyQfAAAAOIZmz56t5ORkhYeHKz09XYWFhS3OXb16tS677DIlJyfLYrEoLy/viO7JIh8AAACG5na13RGoefPmKTs7WzNmzFBxcbGGDx+usWPHqry83O/8AwcOqE+fPnrooYeUkJBwxO8Ji3wAAADgGJk1a5YmT56srKwsDR48WHPmzFFkZKTmzp3rd/6oUaP06KOP6qqrrpLdbj/i+9KTDwAAAENzu9uuJ9/hcMjhcHiN2e12vwvyhoYGFRUVKScnxzNmtVqVmZmpgoKCY1onST4AAADQSrm5uYqKivI6cnNz/c6trKyU0+lUfHy813h8fLxKS0uPaZ0k+QAAADC0ttwnPycnR9nZ2V5jv6at5lhhkQ8AAAC0UkutOf7ExsbKZrOprKzMa7ysrOxXPVTbGrTrAAAAwNDcLkubHYEICwtTamqq8vPzPWMul0v5+fnKyMg42m+DF5J8AAAA4BjJzs7WpEmTNHLkSKWlpSkvL0+1tbXKysqSJE2cOFFJSUmevv6Ghgb98MMPnv+8Y8cOrVixQh07dlS/fv1afV8W+QAAAMAxMmHCBFVUVGj69OkqLS1VSkqKFi5c6HkYt6SkRFbrweaanTt3asSIEZ6vH3vsMT322GM6/fTTtWTJklbfl0U+AAAADM3Vhg/eHokpU6ZoypQpfl87dOGenJwst9v9q+9JTz4AAABgMiT5AAAAMDSX89cn32ZDkg8AAACYDEk+AAAADK299+QHA0k+AAAAYDIk+QAAADA0p4ue/EOR5AMAAAAmQ5IPAAAAQ3M5g11B+0OSDwAAAJgMST4AAAAMzUVPvg+SfAAAAMBkSPIBAABgaPTk+yLJBwAAAEyGJB8AAACGRk++L5J8AAAAwGRI8gEAAGBoLlewK2h/SPIBAAAAkyHJBwAAgKG5nPTkH4okHwAAADAZFvkAAACAydCuAwAAAENz8uCtD5J8AAAAwGRI8gEAAGBoPHjriyQfAAAAMBmSfAAAABgaH4bliyQfAAAAMBmSfAAAABiay0VP/qFI8gEAAACTIckHAACAobmcwa6g/SHJBwAAAEyGJB8AAACGRk++L5J8AAAAwGRI8gEAAGBoTvbJ90GSDwAAAJgMST4AAAAMzeWkJ/9QJPkAAACAyZDkAwAAwNBc9OT7IMkHAAAATIZFPgAAAGAytOsAAADA0Hjw1hdJPgAAAGAyJPkAAAAwNB689UWSDwAAAJgMST4AAAAMzeWiJ/9QJPkAAACAyZDkAwAAwNCczmBX0P6Q5AMAAAAmQ5IPAAAAQ6Mn3xdJPgAAAGAyLPIBAABgaC5n2x1HYvbs2UpOTlZ4eLjS09NVWFh42PlvvfWWBg4cqPDwcJ1wwglasGBBwPdkkQ8AAAAcI/PmzVN2drZmzJih4uJiDR8+XGPHjlV5ebnf+cuWLdPVV1+t3//+9/r22281fvx4jR8/XqtWrQrovizyAQAAYGgul7vNjkDNmjVLkydPVlZWlgYPHqw5c+YoMjJSc+fO9Tv/iSee0Lnnnqs//elPGjRokB544AGdeOKJevLJJwO6L4t8AAAAoJUcDodqamq8DofD4XduQ0ODioqKlJmZ6RmzWq3KzMxUQUGB33MKCgq85kvS2LFjW5zfknazu06fDz8Idgnww+FwKDc3Vzk5ObLb7cEuB4d4KdgFoEX83Wnvrgt2AfCDvzc4Ulmu/Da713333aeZM2d6jc2YMUP33Xefz9zKyko5nU7Fx8d7jcfHx2vt2rV+r19aWup3fmlpaUB1kuTjsBwOh2bOnNniT6gA/OPvDhA4/t7ACHJyclRdXe115OTkBLssH+0myQcAAADaO7vd3urfNMXGxspms6msrMxrvKysTAkJCX7PSUhICGh+S0jyAQAAgGMgLCxMqampys8/2E7kcrmUn5+vjIwMv+dkZGR4zZekjz/+uMX5LSHJBwAAAI6R7OxsTZo0SSNHjlRaWpry8vJUW1urrKwsSdLEiROVlJSk3NxcSdLUqVN1+umn669//asuuOACvfHGG/rmm2/07LPPBnRfFvk4LLvdrhkzZvAAFBAg/u4AgePvDcxowoQJqqio0PTp01VaWqqUlBQtXLjQ83BtSUmJrNaDzTWjR4/Wa6+9pnvuuUd33323jj/+eL333nsaOnRoQPe1uN3uwDf8BAAAANBu0ZMPAAAAmAyLfAAAAMBkWOQDAAAAJsMiHwAAADAZFvkAAACAybDIx2HNnj1bycnJCg8PV3p6ugoLC4NdEtCuff7557rooovUvXt3WSwWvffee8EuCWj3cnNzNWrUKHXq1ElxcXEaP3681q1bF+yyAENjkY8WzZs3T9nZ2ZoxY4aKi4s1fPhwjR07VuXl5cEuDWi3amtrNXz4cM2ePTvYpQCG8dlnn+nWW2/V8uXL9fHHH6uxsVHnnHOOamtrg10aYFjsk48Wpaena9SoUXryySclNX8Mc48ePXTbbbfprrvuCnJ1QPtnsVj07rvvavz48cEuBTCUiooKxcXF6bPPPtNpp50W7HIAQyLJh18NDQ0qKipSZmamZ8xqtSozM1MFBQVBrAwAYHbV1dWSpJiYmCBXAhgXi3z4VVlZKafT6fnI5f+Kj49XaWlpkKoCAJidy+XS7bffrpNPPllDhw4NdjmAYYUEuwAAAID/uvXWW7Vq1SotXbo02KUAhsYiH37FxsbKZrOprKzMa7ysrEwJCQlBqgoAYGZTpkzRv//9b33++ec67rjjgl0OYGi068CvsLAwpaamKj8/3zPmcrmUn5+vjIyMIFYGADAbt9utKVOm6N1339Unn3yi3r17B7skwPBI8tGi7OxsTZo0SSNHjlRaWpry8vJUW1urrKysYJcGtFv79+/Xxo0bPV//+OOPWrFihWJiYtSzZ88gVga0X7feeqtee+01zZ8/X506dfI8+xUVFaWIiIggVwcYE1to4rCefPJJPfrooyotLVVKSor+9re/KT09PdhlAe3WkiVLdMYZZ/iMT5o0SS+++GLbFwQYgMVi8Tv+wgsv6Prrr2/bYgCTYJEPAAAAmAw9+QAAAIDJsMgHAAAATIZFPgAAAGAyLPIBAAAAk2GRDwAAAJgMi3wAAADAZFjkAwAAACbDIh8AAAAwGRb5AAAAgMmwyAcAAABMhkU+AAAAYDL/H6pOXp/OXezFAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# raise AssertionError(\"Stop here\")\n",
    "# alex net acc F1 = 0.62 loss = 0.923\n",
    "# resnet18 acc F1 = ? loss = 1\n",
    "# resnet50 acc F1 = 0.29 loss = 3\n",
    "# resnet50 Custom acc F1 = 0.62 loss = 0.957\n",
    "# resnet50 Custom acc oversampled F1 = 0.78 loss = 0.759\n",
    "# mobilenet \n",
    "# effnet\n",
    "# Vits \n",
    "\n",
    "# model = alexNetNonMod.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\n",
    "\n",
    "\n",
    "def testConfMatrix(self) -> None:\n",
    "        y_hat = torch.tensor([y.argmax() for y in self.bestValPreds]).to('cpu')\n",
    "        y = self.bestValLabels.to('cpu')\n",
    "\n",
    "        print(y_hat),print(y)\n",
    "\n",
    "        confusion_matrix = MulticlassConfusionMatrix(num_classes=3,normalize='true')\n",
    "        print(confusion_matrix(y_hat, y))\n",
    "\n",
    "        confusion_matrix_computed = confusion_matrix.compute().detach().cpu().numpy()\n",
    "\n",
    "        df_cm = pd.DataFrame(confusion_matrix_computed)\n",
    "        plt.figure(figsize = (10,7))\n",
    "        fig_ = sns.heatmap(df_cm, annot=True, cmap='Spectral',fmt='g').get_figure()\n",
    "        plt.show()\n",
    "       \n",
    "\n",
    "testConfMatrix(modelList[3])\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',  # Metric to monitor\n",
    "#     patience=3,          # Number of epochs with no improvement after which training will be stopped\n",
    "#     verbose=True,        # Verbosity mode\n",
    "#     mode='min'           # Mode can be 'min', 'max', or 'auto'\n",
    "# )\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "# trainer = L.Trainer(max_epochs = 20,accelerator='gpu', devices='auto', precision='16-mixed',callbacks=[early_stopping],logger=logger)\n",
    "# trainer.fit(model=alexNetNonMod,train_dataloaders=trainLoader,val_dataloaders=valLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentModel(\n",
       "  (model): MobileNetV3(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): Conv2dNormActivation(\n",
       "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=960, out_features=1280, bias=True)\n",
       "      (1): Hardswish()\n",
       "      (2): Dropout(p=0.2, inplace=True)\n",
       "      (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (linear3): Linear(in_features=32, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "valOutput = []\n",
    "valLabel = []\n",
    "\n",
    "from itertools import tee\n",
    "testTrainer =  L.Trainer(accelerator='gpu', devices='auto')\n",
    "out = testTrainer.predict(alexNet, valLoader)\n",
    "\n",
    "loader1, loader2 = tee(valLoader)#BUG BUG BUG INCREDIBLY MEMORY INEFFICIENT DONOT RUN THIS ON ANYTHIGN BIGGER THAN 1000 IMAGES\n",
    "\n",
    "for idx,x in enumerate(valLoader):\n",
    "    data,label = x\n",
    "    [valLabel.append(x) for x in label]\n",
    "\n",
    "valOutput = trainer.predict(alexNet, loader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
