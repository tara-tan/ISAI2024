{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import torch\n",
    "from abc import ABC,abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "def getAllPaths(path:str, excludeList = []):\n",
    "    path = path\n",
    "    pathList = []\n",
    "    labelList = []\n",
    "    dirList = os.listdir(path)[:3]\n",
    "    for idx, x in enumerate(dirList):\n",
    "        for xx in os.listdir(f\"{path}/{x}\"):\n",
    "            if not any(xxx in xx for xxx in excludeList):\n",
    "                pathList.append(f\"{path}/{x}/{xx}\")\n",
    "                labelList.append(idx)\n",
    "    return pathList, labelList\n",
    "\n",
    "trainPath = r\".\\ExperimentSet\"\n",
    "valPath = r\".\\ValidationSet\"\n",
    "originalPath = r\".\\Osteoporosis Knee X-ray\"\n",
    "\n",
    "trainPathList,trainLabelList = getAllPaths(trainPath)\n",
    "trainPathListNoAug,trainLabelListNoAug = getAllPaths(trainPath,['Rp','Rn','Sup','Sdown','Sleft','Sright'])\n",
    "trainPathListRotate,trainLabelListRotate = getAllPaths(trainPath,['Rp','Rn'])\n",
    "\n",
    "valPathList,valLabelList = getAllPaths(valPath)\n",
    "originalPathList,originalLabelList = getAllPaths(originalPath)\n",
    "\n",
    "trainDirList = os.listdir(trainPath)[:3]\n",
    "valDirList = os.listdir(valPath)[:3]\n",
    "print(trainPathList[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check val distribution (this should be 10,10,10)\n",
    "#this does not account for excluded names\n",
    "trainDist = []\n",
    "for idx,x in enumerate(valDirList):\n",
    "    print(f\"val {x}: {len(os.listdir(f'{valPath}/{x}'))}\")\n",
    "\n",
    "for idx,x in enumerate(trainDirList):\n",
    "    print(f\"train {x}: {len(os.listdir(f'{trainPath}/{x}'))}\")\n",
    "    \n",
    "    trainDist.append(len(os.listdir(f'{trainPath}/{x}')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check image sizes\n",
    "from PIL import Image\n",
    "imageSizes = {}\n",
    "for x in trainPathList:\n",
    "    img = Image.open(x).size\n",
    "    try:\n",
    "        imageSizes[str(img)] = imageSizes[str(img)] + 1\n",
    "    except KeyError:\n",
    "        imageSizes[str(img)] = 1\n",
    "imageSizes #varied image sizes, have to resize to 1024,1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms.functional as vF\n",
    "\n",
    "class OsteoTorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, itemsPath:list, labels:list, transform=None): \n",
    "        \n",
    "        self.itemsPath = itemsPath\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itemsPath)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        try:\n",
    "            image = Image.open(self.itemsPath[idx]).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(idx)\n",
    "            print(self.itemsPath)\n",
    "            raise e\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "        \n",
    "        image.to(device = torch.device('cuda'))\n",
    "\n",
    "        return image, self.labels[idx], self.itemsPath[idx]          \n",
    "    \n",
    "transformCrop = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.RandomResizedCrop((224,224),scale=(0.8,1.0)),\n",
    "                                            # torchvision.transforms.Resize((224,224)),\n",
    "                                            # torchvision.transforms.CenterCrop((224,224)),\n",
    "                                            torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "                                            ])                      \n",
    "     \n",
    "transformNone = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Resize((224,224)),\n",
    "                                            # torchvision.transforms.CenterCrop((224,224)),\n",
    "                                            torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "                                            ])                      \n",
    "\n",
    "transformCenter = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Resize((269,269)),\n",
    "                                            torchvision.transforms.CenterCrop((224,224)),\n",
    "                                            torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "                                            ])                      \n",
    "\n",
    "\n",
    "valTransform = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            # torchvision.transforms.Resize((224,224)),\n",
    "                                            # torchvision.transforms.RandomHorizontalFlip(p=1),\n",
    "                                            torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "                                            ])                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to save Processed images to disk to save on processing time\n",
    "# import PIL\n",
    "# trainDataset = OsteoTorchDataset(trainPathList,trainLabelList,transform)\n",
    "# for x in trainDataset:\n",
    "#     arr = ((x[0].permute(1,2,0)*255).numpy().astype(np.uint8))\n",
    "#     img = PIL.Image.fromarray(arr,mode=\"RGB\")\n",
    "#     img.save(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainLabelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "#inits dataset and dataloader + resamples training data to balance classes\n",
    "def balanceSet(trainPathList,trainLabelList,transform,replacement = False,offset = 3):\n",
    "    trainDataset = OsteoTorchDataset(trainPathList,trainLabelList,transform)      \n",
    "    unique_elements, counts = torch.unique(torch.tensor(trainLabelList), return_counts=True)\n",
    "    sampleWeights = 1. / counts.float() #I HAVE NO IDEA WHY THIS WORKS BUT IT DOES\n",
    "    # MORAL OF THE STORY: FOLLOW THE FUCKING TUTORIAL DONT TRY CHANGING SHIT ON YOUR OWN ***EVEN IF THE DOCUMENTATION SAYS YOU SHOULD***\n",
    "    # trainSampler = WeightedRandomSampler(weights=[sampleWeights[x] for x in trainLabelList],num_samples=len(trainDataset),replacement=True)\n",
    "\n",
    "    # trainSampler = WeightedRandomSampler(weights=[sampleWeights[x] for x in trainLabelList],num_samples=min(trainDist)*8,replacement=False)\n",
    "\n",
    "    trainSampler = WeightedRandomSampler(weights=[sampleWeights[x] for x in trainLabelList],num_samples=counts.min().item()*(3+offset),replacement= replacement)\n",
    "\n",
    "    trainLoader = DataLoader(trainDataset, batch_size = 8,shuffle=False,num_workers=0,sampler=trainSampler)\n",
    "    return trainLoader\n",
    "\n",
    "trainLoader = balanceSet(trainPathList,trainLabelList,transformCenter)\n",
    "\n",
    "valDataset = OsteoTorchDataset(valPathList,valLabelList,valTransform)\n",
    "valLoader = DataLoader(valDataset, batch_size = 8,shuffle=False,num_workers=0)\n",
    "\n",
    "unique_elements, counts = torch.unique(torch.cat([y for x,y,z in trainLoader]), return_counts=True)#delete this\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, counts = torch.unique(torch.cat([x for y,x,z in valLoader]), return_counts=True)#delete this\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = next(iter(trainLoader))\n",
    "index = -1\n",
    "\n",
    "temp[0][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp[0][0].min()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "index += 1\n",
    "# print(temp[1][index],temp[2][index])\n",
    "print((temp[0][index].numpy()*255).max().astype(np.uint8))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "print(str(temp[2][index]))\n",
    "# Plot the first image\n",
    "axs[0].imshow(((temp[0][index]+1)/2).permute(1, 2, 0))\n",
    "axs[0].set_title('Image 1')\n",
    "\n",
    "# Plot the second image\n",
    "axs[1].imshow(Image.open(temp[2][index]).convert('RGB'))\n",
    "axs[1].set_title('Image 2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL PREP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models here\n",
    "import sklearn.metrics\n",
    "from resnetModel import ResNet50\n",
    "import sklearn\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torcheval.metrics.functional import multiclass_f1_score,multiclass_confusion_matrix,multiclass_accuracy\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from torch import mode\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from abc import ABC\n",
    "import typing\n",
    "\n",
    "#FIXME clean up my fucking code ffs its so UGLY\n",
    "\n",
    "class ExperimentModel(L.LightningModule,ABC):\n",
    "\n",
    "    existingModels = []\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        ExperimentModel.existingModels.append(self)\n",
    "        super().__init__()\n",
    "\n",
    "        #init the models here in a subclass\n",
    "        self.name = self.__class__.__name__\n",
    "        self.num_class = 3\n",
    "        self.classWeight = torch.tensor([0.204, 0.052, 0.175],device='cuda')\n",
    "        self.LOGPATH = None\n",
    "        self.trainLoader = None\n",
    "        self.valLoader = None\n",
    "        \n",
    "        # if (torch.tensor([0.204, 0.052, 0.175],device='cuda')*0).detach()[0].item() == 0:\n",
    "        #   self.classWeight = torch.ones(self.classWeight.size(),device='cuda')\n",
    "\n",
    "        self.dump = []\n",
    "\n",
    "    def setLogPath(self,logPath): # this should be the general logging dir of the model's current version (refer to tblogger in training loop)\n",
    "        self.LOGPATH = logPath\n",
    "    \n",
    "    def setDataLoader(self,trainLoader:typing.Optional[DataLoader] = None,valLoader:typing.Optional[DataLoader] = None):\n",
    "        self.trainLoader = trainLoader\n",
    "        self.valLoader = valLoader  \n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward():\n",
    "        pass\n",
    "    \n",
    "    def training_step(self,batch):\n",
    "        data,label,path = batch\n",
    "        output = self(data)\n",
    "        self.dump.append([path,label])\n",
    "        # loss = F.cross_entropy(output,label,weight=self.classWeight)\n",
    "        loss = F.cross_entropy(output,label)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc\", multiclass_accuracy(output.argmax(1),label,num_classes=self.num_class), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> None:\n",
    "        data,label,path = batch\n",
    "        output = self(data)\n",
    "\n",
    "        preds = output.argmax(1)\n",
    "        loss = F.cross_entropy(output,label)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_F1\", sklearn.metrics.f1_score(label.cpu(),preds.cpu(),labels = range(self.num_class),average = 'macro'), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", multiclass_accuracy(preds,label,num_classes=self.num_class), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    \n",
    "    # def on_validation_epoch_end(self) -> None:\n",
    "\n",
    "    #     accuracy = torch.stack(self.valScore).mean()\n",
    "    #     self.bestValScore.append(accuracy)\n",
    "    #     self.bestValPreds.append(torch.cat(self.valPreds))\n",
    "    #     self.bestValLabels.append(torch.cat(self.valLabels))\n",
    "        # self.log(\"val_acc_F1\", multiclass_f1_score(torch.cat(self.valPreds),torch.cat(self.valLabels),num_classes=self.num_class,average = 'macro'), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "       \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n",
    "        return optimizer\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        data,label = batch\n",
    "        return self(data.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import re\n",
    "\n",
    "import torchvision.transforms.functional\n",
    "import resnetModel\n",
    "import shutil\n",
    "\n",
    "class GradCamTrack(ExperimentModel): #Class for GradCam visualization, should be subclassed by the model to be visualized with a new init and a forward + hook\n",
    "    \n",
    "    def __init__(self,) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gradients = None\n",
    "        self.cachedActivation = None\n",
    "\n",
    "    # cachedActivation is no longer needed, but kept for legacy purposes\n",
    "    def activations_hook(self, grad, imageActivation = None):\n",
    "        if imageActivation != None:\n",
    "            self.cachedActivation = imageActivation\n",
    "        self.gradients = grad\n",
    "\n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)\n",
    "     \n",
    "    # Runs CAM and overlays the image\n",
    "    # this is the function to call for visualization\n",
    "    def visualize(self,dataloader:torch.utils.data.dataloader.DataLoader): #TODO FIX THIS for generalization\n",
    "        \n",
    "        with torch.inference_mode(False):\n",
    "            out,path,pred = self.CAM(dataloader)\n",
    "        \n",
    "        #visualize saves the images in 2 locations, one for the current epoch(same path as logdir) and one for the latest model (visualizations/gradCam)\n",
    "       \n",
    "        try:\n",
    "            os.mkdir(rf'./{self.LOGPATH}/gradCam')\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            shutil.rmtree(rf'./{self.LOGPATH}/gradCam/{self.current_epoch}')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        \n",
    "        os.mkdir(rf'./{self.LOGPATH}/gradCam/{self.current_epoch}')\n",
    "\n",
    "        for i in range(len(out)):\n",
    "            self.visualizeAndWrite(out[i],path[i],pred[i],rf'./{self.LOGPATH}/gradCam/{self.current_epoch}')\n",
    "        return pred,path\n",
    "    \n",
    "    # Visualizes the Gradients of the last conv layer of model\n",
    "    # can take in torch dataloader or ([img],[label],[path])\n",
    "    def CAM(self,dataloader:torch.utils.data.dataloader.DataLoader):\n",
    "        \n",
    "        self.eval()\n",
    "        self.requires_grad_()\n",
    "        predList = []\n",
    "\n",
    "        pathList = []\n",
    "        \n",
    "        heatmapList = []\n",
    "\n",
    "        for x in dataloader:\n",
    "            img,label,path = x\n",
    "\n",
    "            img = img.to(device = torch.device('cuda'))\n",
    "            for i in range(len(img)):\n",
    "                out = self(torch.unsqueeze(img[i],0).requires_grad_())\n",
    "                pred= out.argmax(1).item()\n",
    "                predList.append(pred)\n",
    "                # get the gradient of the output with respect to the parameters of the model\n",
    "                out[:, pred].backward()\n",
    "\n",
    "                # pull the gradients out of the model\n",
    "                gradients = self.get_activations_gradient()\n",
    "\n",
    "                # pool the gradients across the channels\n",
    "                pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "                # get the activations of the last convolutional layer\n",
    "                if self.cachedActivation != None:\n",
    "                    activations = self.cachedActivation.detach()\n",
    "                else:\n",
    "                    activations = self.get_activations(torch.unsqueeze(img[i],0)).detach() # DONT forget to apply image changes here too\n",
    "                # weight the channels by corresponding gradients\n",
    "                for j in range(pooled_gradients.shape[0]):\n",
    "                    activations[:, j, :, :] *= pooled_gradients[j]\n",
    "                    \n",
    "                # average the channels of the activations\n",
    "                heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "                # relu on top of the heatmap\n",
    "                # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "                heatmap = np.maximum(heatmap.detach().cpu(), 0)\n",
    "\n",
    "                # normalize the heatmap\n",
    "                heatmap /= torch.max(heatmap)\n",
    "\n",
    "                # draw the heatmap\n",
    "                heatmapList.append(heatmap)\n",
    "                \n",
    "                pathList.append(path[i])\n",
    "\n",
    "                self.zero_grad()\n",
    "\n",
    "        return heatmapList,pathList,predList\n",
    "    \n",
    "\n",
    "    #writes files to disk for visualization, format: model_epoch_label_pred/image.jpg\n",
    "    #logPath should be the \"{logging directory of the model}/gradCam/epoch\"\n",
    "    def visualizeAndWrite(self,out:torch.tensor,path:str,pred:str,logPath:str): #TODO Implement this with logger\n",
    "\n",
    "        labelDict = {0:'N',1:'OP',2:'OS'}\n",
    "            \n",
    "        img = cv2.imread(path)\n",
    "        heatmap = cv2.resize(out.numpy(), (img.shape[1], img.shape[0]))\n",
    "        heatmap = np.uint8(255 * heatmap)\n",
    "        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "        # print(max(heatmap.flatten()),min(heatmap.flatten()))\n",
    "        superimposed_img = heatmap * 0.4 + img\n",
    "        # superimposed_img = superimposed_img / np.max(superimposed_img)\n",
    "        # self.logger.experiment.add_image(f'{path.split(r\"/\")[-1]}',superimposed_img,self.current_epoch,dataformats = 'HWC')\n",
    "        cv2.imwrite(rf'{logPath}/pred{labelDict[pred]}_{path.split(r\"/\")[-1].split(r\".\")[0]}.jpg', superimposed_img)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        super().on_validation_epoch_end()\n",
    "        pred,path = self.visualize(valLoader)\n",
    "        label = [x.split(r\"/\")[-2] for x in path]\n",
    "        labelDict = {'normal':0,'osteopenia':1,'osteoporosis':2}\n",
    "        \n",
    "        label = [labelDict[x] for x in label]\n",
    "        # self.log(\"classification report\",classification_report(label,pred))\n",
    "        print(classification_report(label,pred))\n",
    "        plt.clf()\n",
    "        cm= confusion_matrix(label,pred,labels=[0,1,2],normalize='true')\n",
    "        sns.heatmap(cm, annot=True, cmap='Blues', fmt='.2f',)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        fig = plt.gcf()\n",
    "        img = Image.frombytes('RGB', \n",
    "            fig.canvas.get_width_height(),fig.canvas.tostring_rgb())\n",
    "        grid = torchvision.utils.make_grid(torchvision.transforms.functional.pil_to_tensor(img)) \n",
    "        self.logger.experiment.add_image('Confusion Matrix',grid,self.current_epoch)\n",
    "        # self.log({'Confusion Matrix': plt.gcf()}) #watch out for BUG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGGModel(GradCamTrack):\n",
    "    def __init__(self,vgg:torchvision.models.vgg.VGG) -> None:\n",
    "        super().__init__()\n",
    "        self.features_conv = vgg.features[:36]\n",
    "        # get the max pool of the features stem\n",
    "        self.max_pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        # get the classifier of the vgg19\n",
    "        self.classifier = vgg.classifier\n",
    "        self.classifier[6] = torch.nn.Linear(4096,self.num_class)\n",
    "\n",
    "\n",
    "    def forward(self,inTensor):\n",
    "        x = self.features_conv(inTensor)\n",
    "        \n",
    "        # # register the hook\n",
    "        if x.requires_grad:\n",
    "            hook = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        x = self.max_pool(x)\n",
    "        x = x.flatten(1,-1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class ResnetModel(GradCamTrack):\n",
    "    def __init__(self,resnet:torchvision.models.resnet.ResNet) -> None:\n",
    "        super().__init__()\n",
    "        self.features_conv = torch.nn.Sequential(*[x for x in resnet.children()][:-2])\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = torch.nn.Linear(2048, self.num_class)\n",
    "        # self.model.layer4[2] = resnetModel.CustBottleneck(2048,512,self.activations_hook)\n",
    "        # self.model.fc = torch.nn.Linear(2048,3)\n",
    "        \n",
    "    def forward(self,inTensor:torch.tensor):\n",
    "        x = self.features_conv(inTensor)\n",
    "        \n",
    "        if x.requires_grad:\n",
    "            hook = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class EfficientNetModel(GradCamTrack):\n",
    "    def __init__(self,effNet:torchvision.models.efficientnet) -> None:\n",
    "        super().__init__()\n",
    "        self.features_conv = effNet.features\n",
    "        self.avgpool = effNet.avgpool\n",
    "        self.classifier = effNet.classifier\n",
    "        self.classifier[1] = torch.nn.Linear(1280,self.num_class)\n",
    "        \n",
    "        \n",
    "    def forward(self,inTensor:torch.tensor):\n",
    "        x = self.features_conv(inTensor)\n",
    "        \n",
    "        if x.requires_grad:\n",
    "            hook = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init models here\n",
    "\n",
    "ExperimentModel.existingModels = []\n",
    "# resnet101 = ResnetModel(torch.hub.load('pytorch/vision:v0.19.0', 'resnet101', pretrained=True))\n",
    "\n",
    "vgg191 = VGGModel(torchvision.models.vgg19(pretrained=True))\n",
    "vgg191.setDataLoader(balanceSet(trainPathList,trainLabelList,transformNone),valLoader)\n",
    "vgg191.name = 'VGGModelNoTransform'\n",
    "\n",
    "vgg192 = VGGModel(torchvision.models.vgg19(pretrained=True))\n",
    "vgg192.setDataLoader(balanceSet(trainPathList,trainLabelList,transformCrop),valLoader)\n",
    "vgg192.name = 'VGGModelRandCrop'\n",
    "\n",
    "vgg193 = VGGModel(torchvision.models.vgg19(pretrained=True))\n",
    "vgg193.setDataLoader(balanceSet(trainPathList,trainLabelList,transformCenter),valLoader)\n",
    "vgg193.name = 'VGGModelCenterCrop'\n",
    "\n",
    "vgg194 = VGGModel(torchvision.models.vgg19(pretrained=True))\n",
    "vgg194.setDataLoader(balanceSet(trainPathListNoAug,trainLabelListNoAug,transformNone),valLoader)\n",
    "vgg194.name = 'VGGModelNoAugNoTransform'\n",
    "\n",
    "\n",
    "vgg195 = VGGModel(torchvision.models.vgg19(pretrained=True))\n",
    "vgg195.setDataLoader(balanceSet(trainPathListNoAug,trainLabelListNoAug,transformCenter),valLoader)\n",
    "vgg195.name = 'VGGModelNoAugCenterCrop'\n",
    "\n",
    "vgg195 = VGGModel(torchvision.models.vgg19(pretrained=True))\n",
    "vgg195.setDataLoader(balanceSet(trainPathListNoAug,trainLabelListNoAug,transformNone,replacement=True,offset=9),valLoader)\n",
    "vgg195.name = 'VGGModelBalanced'\n",
    "\n",
    "\n",
    "# efficientNetv2 = EfficientNetModel(torch.hub.load('pytorch/vision:v0.19.0', 'efficientnet_v2_l', pretrained=True))\n",
    "[x.name for x in ExperimentModel.existingModels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "import gc\n",
    "import warnings\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.profilers import AdvancedProfiler\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message = r\".*ill-defined and being set to 0.0\")\n",
    "                            \n",
    "    for x in ExperimentModel.existingModels:\n",
    "        \n",
    "        try:\n",
    "            os.mkdir(f\"./lightning_logs/{x.name}\")\n",
    "            \n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        logDirectory = f\"./lightning_logs/{x.name}\"\n",
    "        versionList = os.listdir(logDirectory)\n",
    "        versionList.sort(key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "        try:\n",
    "            buffer = int(re.search(r'\\d+', versionList[-1]).group())\n",
    "            versionToLog = (f'version_{buffer+1}')\n",
    "\n",
    "        except IndexError:\n",
    "            versionToLog = 'version_0'\n",
    "\n",
    "        LOGPATH = r'/'.join([logDirectory,versionToLog]) # this is based on the tbLogger path scheme\n",
    "\n",
    "        x.setLogPath(LOGPATH)\n",
    "            \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_acc_F1',  # Metric to monitor\n",
    "            patience=10,          # Number of epochs with no improvement after which training will be stopped\n",
    "            verbose=False,        # Verbosity mode\n",
    "            mode='max'           # Mode can be 'min', 'max', or 'auto'\n",
    "        )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(dirpath=LOGPATH, save_top_k=3, monitor=\"val_acc_F1\",mode='max') #idk why but no, checkpoint_callback cannot infer logPath from tblogger\n",
    "\n",
    "        profiler = AdvancedProfiler(dirpath=LOGPATH, filename=\"profiler_logs\")\n",
    "        \n",
    "        tbLogger = TensorBoardLogger(\"lightning_logs\",name=f\"{x.name}\") \n",
    "\n",
    "        trainer = L.Trainer(\n",
    "                            profiler = \"simple\",\n",
    "                            # profiler = profiler,\n",
    "                            max_epochs = 75,\n",
    "                            min_epochs = 50,\n",
    "                            accelerator ='gpu', \n",
    "                            devices = 'auto', \n",
    "                            precision = '16-mixed',\n",
    "                            logger = tbLogger,\n",
    "                            callbacks = [early_stopping,checkpoint_callback],\n",
    "                        )\n",
    "        \n",
    "        trainer.fit(model=x,train_dataloaders=(x.trainLoader if x.trainLoader != None else trainLoader),val_dataloaders=(x.valLoader if x.valLoader != None else valLoader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "models = [x.name for x in ExperimentModel.existingModels][5]\n",
    "logDirectory = f\"./lightning_logs/{models}\"\n",
    "versionList = os.listdir(logDirectory)\n",
    "versionList.sort(key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "# print(versionList[-1])\n",
    "tbDir = f\"{logDirectory}/{versionList[-1]}\"\n",
    "!echo $tbDir\n",
    "!taskkill /F /IM tensorboard.exe    \n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $tbDir --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradCam Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.19.0', 'resnet50', pretrained=True)\n",
    "resnetPath = r'C:\\Users\\assaw\\Documents\\c_stuff\\Python\\machine learning\\ISAIConference\\tb_logs\\ResnetModel\\version_22\\checkpoints'\n",
    "resnet101 = ResnetModel.load_from_checkpoint('\\\\'.join([resnetPath,os.listdir(resnetPath)[-1]]),resnet = model)\n",
    "torch.cuda.empty_cache()\n",
    "resnet101.to(device = torch.device('cuda'))\n",
    "# out,label = resnet50.visualize(valLoader)\n",
    "resnet101.on_validation_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = VGGModel.load_from_checkpoint(r'tb_logs\\VGGModel\\version_26\\checkpoints\\epoch=19-step=720.ckpt',vgg = torchvision.models.vgg19(pretrained=False))\n",
    "outvgg,path = model.visualize(valLoader)\n",
    "\n",
    "plt.imshow(outvgg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg19\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic pytorch for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.cuda.empty_cache()\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([0.204, 0.052, 0.175],device='cuda'))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "accHistory = []\n",
    "lossHistory = []\n",
    "valAccHistory = []\n",
    "valLabs = []\n",
    "valPreds = []\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = []\n",
    "    \n",
    "    # Iterate over the training dataset\n",
    "    for inputs, labels, _ in trainLoader:\n",
    "        # Move the inputs and labels to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs.float())\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "        accuracy = torch.sum(predicted_labels == labels).item() / labels.size(0)\n",
    "        running_accuracy.append(accuracy)\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print the average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss / len(trainLoader)}\")\n",
    "    print(F\"Accuracy: {sum(running_accuracy)/len(running_accuracy)}\")\n",
    "    accHistory.append(sum(running_accuracy)/len(running_accuracy))\n",
    "    lossHistory.append(running_loss / len(trainLoader))\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        runningValAccHistory = []\n",
    "        runningValLabs = []\n",
    "        runningValPreds = []\n",
    "        for inputs, labels, _ in valLoader:\n",
    "\n",
    "            # Move the inputs and labels to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs.float())\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute the predicted labels\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Compute the accuracy\n",
    "            accuracy = torch.sum(predicted_labels == labels).item() / labels.size(0)\n",
    "\n",
    "            runningValAccHistory.append(accuracy)\n",
    "            runningValLabs.append(labels)\n",
    "            runningValPreds.append(predicted_labels)\n",
    "\n",
    "        # print(f\"Validation Loss: {loss.item()}\")\n",
    "        valPreds.append(runningValPreds)\n",
    "        valLabs.append(runningValLabs)\n",
    "        print(f\"Validation Accuracy: {sum(runningValAccHistory)/len(valLoader)}\")\n",
    "        valAccHistory.append(sum(runningValAccHistory)/len(valLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "idx = 25\n",
    "predsTest = torch.cat(valPreds[idx-1])\n",
    "labelsTest = torch.cat(valLabs[idx-1])\n",
    "print(valAccHistory[idx-1])\n",
    "print(classification_report(predsTest.cpu(),labelsTest.cpu()))\n",
    "cm= confusion_matrix(predsTest.cpu(),labelsTest.cpu(),normalize='true')\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='.2f',)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe with the data\n",
    "df = pd.DataFrame({'Epoch': range(len(accHistory)), 'Accuracy': accHistory, 'Loss': lossHistory, 'Validation Accuracy': valAccHistory})\n",
    "\n",
    "# Create the line plot\n",
    "sns.relplot(data=df, x='Epoch', y='Accuracy', kind='line')\n",
    "sns.relplot(data=df, x='Epoch', y='Loss', kind='line')\n",
    "sns.relplot(data=df, x='Epoch', y='Validation Accuracy', kind='line')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
