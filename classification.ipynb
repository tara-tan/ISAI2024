{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\assaw\\.conda\\envs\\AI\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoImageProcessor, ResNetForImageClassification,ResNetConfig\n",
    "import torch\n",
    "from abc import ABC,abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check if layers are actually being trained\n",
    "#TODO add more models\n",
    "#TODO compare metrics\n",
    "#TODO test out prediction loop\n",
    "#TODO learn how to use tensorboard\n",
    "\n",
    "#TODO day2 Balance Dataset\n",
    "#TODO day2 Implement checkpointing and checkpoint loading\n",
    "#TODO day2 implement an actually good and modular pipeline\n",
    "#TODO day2 EDA + explore augments\n",
    "#TODO day2 normalize and test out mean/median/std of pixel values\n",
    "#TODO day2 Actually implement additional layers and finish what i started\n",
    "#TODO Verify data is sent correctly (esp labels)\n",
    "\n",
    "#TODO day2 implement localization + research more abt osteopenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###DATASET PREP###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectPath = r\"Osteoporosis Knee X-ray\"\n",
    "pathList = []\n",
    "labelList = []\n",
    "dirList = os.listdir(projectPath)[:3]\n",
    "for idx, x in enumerate(dirList):\n",
    "    for xx in os.listdir(f\"{projectPath}/{x}\"):\n",
    "        pathList.append(f\"{projectPath}/{x}/{xx}\")\n",
    "        labelList.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(1024, 1024)': 63,\n",
       " '(2180, 2660)': 61,\n",
       " '(2660, 2180)': 10,\n",
       " '(2430, 1994)': 94,\n",
       " '(2386, 1994)': 1,\n",
       " '(1994, 2430)': 2,\n",
       " '(2430, 1910)': 2,\n",
       " '(2402, 1994)': 1,\n",
       " '(2378, 1994)': 2,\n",
       " '(2362, 1994)': 1,\n",
       " '(2430, 1958)': 1,\n",
       " '(2398, 1994)': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "imageSizes = {}\n",
    "for x in pathList:\n",
    "    img = Image.open(x).size\n",
    "    try:\n",
    "        imageSizes[str(img)] = imageSizes[str(img)] + 1\n",
    "    except KeyError:\n",
    "        imageSizes[str(img)] = 1\n",
    "imageSizes #varied image sizes, have to resize to 1024,1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 154, 49)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelList.count(0),labelList.count(1),labelList.count(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "\n",
    "class OsteoTorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, itemsPath:list, labels:list, transform=None):\n",
    "        \n",
    "        self.itemsPath = itemsPath\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itemsPath)\n",
    "\n",
    "    def __getitem__(self,idx)->tuple[Image.Image,int]:\n",
    "        # if torch.is_tensor(idx):\n",
    "        #     idx = idx.tolist()\n",
    "        image = Image.open(self.itemsPath[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return torchvision.transforms.functional.pil_to_tensor(image), self.labels[idx]\n",
    "        \n",
    "    \n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.Resize([244,244])\n",
    "                                            # ,torchvision.transforms.Grayscale()\n",
    "                                            ])\n",
    "osteoDataset = OsteoTorchDataset(pathList,labelList,transform)\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "train,val = torch.utils.data.random_split(osteoDataset,[0.7,0.3])#MAY BUG\n",
    "\n",
    "trainLoader = DataLoader(train, batch_size = 16,shuffle=False,num_workers=0)\n",
    "valLoader = DataLoader(val, batch_size = 16,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataWrapper import DataWrapper\n",
    "# BUG still dosent work, does not fix the Num_workers issue\n",
    "# loader = DataWrapper(pathList,labelList, batch_size = 16,shuffle=False,num_workers=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models here\n",
    "from models import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\assaw/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\assaw\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\assaw\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torcheval.metrics.functional import multiclass_f1_score,multiclass_confusion_matrix,multiclass_accuracy\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import mode\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "#FIXME implement proper instancing to support multiple models\n",
    "#FIXME identify bottleneck\n",
    "#FIXME clean up my fucking code ffs its so UGLY\n",
    "#FIXME \n",
    "\n",
    "class ExperimentModel(L.LightningModule):\n",
    "    def __init__(self,pretrainedModel,pretrainedPreprocess,outputSize,resize = False) -> None:\n",
    "        super().__init__()\n",
    "        self.model = pretrainedModel\n",
    "        # if pretrainedPreprocess:\n",
    "        #     self.preprocess = pretrainedPreprocess  \n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        self.resize = resize\n",
    "        if self.resize:\n",
    "            self.linear1 = torch.nn.Linear(outputSize, 512)\n",
    "            self.linear2 = torch.nn.Linear(512, 32)\n",
    "            self.linear3 = torch.nn.Linear(32, 3) \n",
    "        self.valLog = []\n",
    "        self.epoch = []\n",
    "        self.valPreds = []\n",
    "        self.valLabels = []\n",
    "        \n",
    "    def forward(self,input):\n",
    "        out = self.model(input)\n",
    "        if self.resize:\n",
    "            out = self.linear1(out)\n",
    "            out = self.linear2(out)\n",
    "            out = self.linear3(out)\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def training_step(self,batch):\n",
    "        data,label = batch\n",
    "        output = self(data.float())\n",
    "        loss= F.cross_entropy(output,label)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data,label = batch\n",
    "        output = self(data.float())\n",
    "        loss= F.cross_entropy(output,label)\n",
    "        self.valLabels.append(label)\n",
    "        self.valPreds.append(output)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc_F1\", multiclass_f1_score(output,label,num_classes=3), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", multiclass_accuracy(output,label,num_classes=3), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    # def predict(self)\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        # self.log(\"cust\",self.linear3.weight.sum(),logger=True)\n",
    "\n",
    "        y_hat = torch.cat(self.valPreds)\n",
    "        y = torch.cat(self.valLabels)\n",
    "\n",
    "        confusion_matrix = MulticlassConfusionMatrix(num_classes=3, threshold=0.05)\n",
    "        confusion_matrix(y_hat, y.int())\n",
    "\n",
    "        confusion_matrix_computed = confusion_matrix.compute().detach().cpu().numpy().astype(int)\n",
    "\n",
    "        df_cm = pd.DataFrame(confusion_matrix_computed)\n",
    "        plt.figure(figsize = (10,7))\n",
    "        fig_ = sns.heatmap(df_cm, annot=True, cmap='Spectral').get_figure()\n",
    "        plt.close(fig_)\n",
    "        self.log.experiment.add_figure(\"Confusion matrix\", fig_, self.current_epoch)\n",
    "\n",
    "    def get_self_variables(self):\n",
    "        linear_variables = [x for x in dir(self) if 'linear' in x]\n",
    "        return [getattr(self, x, None).parameters() for x in linear_variables]\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(),lr=0.001)\n",
    "        # return torch.optim.AdamW(itertools.chain(self.model.parameters(),*self.get_self_variables()), lr=0.001)\n",
    "        pass\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        data,label = batch\n",
    "        return self(data.float())\n",
    "\n",
    "# class ResNetCallbacks(L.Callback): #TODO implement callbacks\n",
    "#     def on_validation_epoch_end(trainer, pl_module):\n",
    "#         valLog.append(sum(self.cumLog)/len(self.cumlog))\n",
    "\n",
    "    \n",
    "# alexNet = ExperimentModel(AlexNet(num_classes=1000),None,1000,True)\n",
    "# alexNetNonMod = ExperimentModel(AlexNet(num_classes=3),None,3)\n",
    "alexNetNonMod = ExperimentModel(model,None,1000,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  45%|████▌     | 5/11 [02:37<03:08,  0.03it/s, v_num=8, train_loss_step=3.320, val_loss=1.250, val_acc_F1=0.592, val_acc=0.592, train_loss_epoch=2.340]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model   | ResNet | 25.6 M | train\n",
      "1 | linear1 | Linear | 512 K  | train\n",
      "2 | linear2 | Linear | 16.4 K | train\n",
      "3 | linear3 | Linear | 99     | train\n",
      "-------------------------------------------\n",
      "26.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.344   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 11/11 [00:09<00:00,  1.15it/s, v_num=10, train_loss_step=2.010, val_loss=73.60, val_acc_F1=0.620, val_acc=0.620, train_loss_epoch=3.160]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 73.561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 11/11 [00:09<00:00,  1.10it/s, v_num=10, train_loss_step=0.932, val_loss=2.350, val_acc_F1=0.620, val_acc=0.620, train_loss_epoch=1.440]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 71.207 >= min_delta = 0.0. New best score: 2.354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 11/11 [00:09<00:00,  1.17it/s, v_num=10, train_loss_step=1.070, val_loss=1.690, val_acc_F1=0.380, val_acc=0.380, train_loss_epoch=1.150]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.666 >= min_delta = 0.0. New best score: 1.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 11/11 [00:09<00:00,  1.17it/s, v_num=10, train_loss_step=0.756, val_loss=1.090, val_acc_F1=0.620, val_acc=0.620, train_loss_epoch=0.948]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.598 >= min_delta = 0.0. New best score: 1.090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 11/11 [00:09<00:00,  1.13it/s, v_num=10, train_loss_step=0.891, val_loss=1.020, val_acc_F1=0.620, val_acc=0.620, train_loss_epoch=0.972]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.069 >= min_delta = 0.0. New best score: 1.021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 11/11 [00:09<00:00,  1.12it/s, v_num=10, train_loss_step=0.764, val_loss=0.957, val_acc_F1=0.620, val_acc=0.620, train_loss_epoch=0.951]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.063 >= min_delta = 0.0. New best score: 0.957\n",
      "`Trainer.fit` stopped: `max_epochs=7` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 11/11 [00:11<00:00,  0.99it/s, v_num=10, train_loss_step=0.764, val_loss=0.957, val_acc_F1=0.620, val_acc=0.620, train_loss_epoch=0.951]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=10,          # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=True,        # Verbosity mode\n",
    "    mode='min'           # Mode can be 'min', 'max', or 'auto'\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "trainer = L.Trainer(max_epochs = 7,accelerator='gpu', devices='auto', precision='16-mixed',callbacks=[early_stopping],logger=logger)\n",
    "# trainer.fit(model=alexNet,train_dataloaders=loader.trainLoader,val_dataloaders=loader.valLoader)\n",
    "trainer.fit(model=alexNetNonMod,train_dataloaders=trainLoader,val_dataloaders=valLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# raise AssertionError(\"Stop here\")\n",
    "# alex net acc F1 = 0.62 loss = 0.923\n",
    "# resnet18 acc F1 = ? loss = 1\n",
    "# resnet50 acc F1 = 0.29 loss = 3\n",
    "# resnet50 Custom acc F1 = 0.62 loss = 0.957\n",
    "\n",
    "\n",
    "# model = alexNetNonMod.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\n",
    "\n",
    "\n",
    "def testConfMatrix(self) -> None:\n",
    "        # self.log(\"cust\",self.linear3.weight.sum(),logger=True)\n",
    "\n",
    "        y_hat = torch.cat(self.valPreds)\n",
    "        y = torch.cat(self.valLabels)\n",
    "\n",
    "        confusion_matrix = MulticlassConfusionMatrix(num_classes=3).to('cuda')\n",
    "        confusion_matrix(y_hat, y.int())\n",
    "\n",
    "        confusion_matrix_computed = confusion_matrix.compute().detach().cpu().numpy().astype(int)\n",
    "\n",
    "        df_cm = pd.DataFrame(confusion_matrix_computed)\n",
    "        plt.figure(figsize = (10,7))\n",
    "        fig_ = sns.heatmap(df_cm, annot=True, cmap='Spectral',fmt='g').get_figure()\n",
    "        plt.show()\n",
    "       \n",
    "\n",
    "testConfMatrix(alexNetNonMod)\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',  # Metric to monitor\n",
    "#     patience=3,          # Number of epochs with no improvement after which training will be stopped\n",
    "#     verbose=True,        # Verbosity mode\n",
    "#     mode='min'           # Mode can be 'min', 'max', or 'auto'\n",
    "# )\n",
    "\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "# trainer = L.Trainer(max_epochs = 20,accelerator='gpu', devices='auto', precision='16-mixed',callbacks=[early_stopping],logger=logger)\n",
    "# trainer.fit(model=alexNetNonMod,train_dataloaders=trainLoader,val_dataloaders=valLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# !kill 27636\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=tb_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "valOutput = []\n",
    "valLabel = []\n",
    "\n",
    "from itertools import tee\n",
    "testTrainer =  L.Trainer(accelerator='gpu', devices='auto')\n",
    "out = testTrainer.predict(alexNet, valLoader)\n",
    "\n",
    "loader1, loader2 = tee(valLoader)#BUG BUG BUG INCREDIBLY MEMORY INEFFICIENT DONOT RUN THIS ON ANYTHIGN BIGGER THAN 1000 IMAGES\n",
    "\n",
    "for idx,x in enumerate(valLoader):\n",
    "    data,label = x\n",
    "    [valLabel.append(x) for x in label]\n",
    "\n",
    "valOutput = trainer.predict(alexNet, loader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
